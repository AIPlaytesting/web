{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Our Goal AI Playtesting aims to create an AI that can playtest combat based PVE card games and help balance it. We expect that the AI will identify dominant strategies or unbalanced cards and generate playtest data to help game designers to do the balancing. To do this we choose Slay the Spire as our target game and we are developing our own machine learning algorithm. We will document our findings and experiment through the semester and publish it afterwards. Our Method We created a tool for designers to balance a game called Slay the Spire. Design : build your deck by adding Slay the Spire cards, or even customize your cards. Training : train the AI agent using Deep Reinforcement Learning algorithms. Playtest : let AI playtest and display statistics for identifying imbalance and updating the rules of the game.","title":"Home"},{"location":"#our-goal","text":"AI Playtesting aims to create an AI that can playtest combat based PVE card games and help balance it. We expect that the AI will identify dominant strategies or unbalanced cards and generate playtest data to help game designers to do the balancing. To do this we choose Slay the Spire as our target game and we are developing our own machine learning algorithm. We will document our findings and experiment through the semester and publish it afterwards.","title":"Our Goal"},{"location":"#our-method","text":"We created a tool for designers to balance a game called Slay the Spire. Design : build your deck by adding Slay the Spire cards, or even customize your cards. Training : train the AI agent using Deep Reinforcement Learning algorithms. Playtest : let AI playtest and display statistics for identifying imbalance and updating the rules of the game.","title":"Our Method"},{"location":"ai/","text":"AI","title":"AI"},{"location":"ai/#ai","text":"","title":"AI"},{"location":"demo/","text":"Application Demo (21 minutes)","title":"Demo"},{"location":"demo/#application-demo-21-minutes","text":"","title":"Application Demo (21 minutes)"},{"location":"design/","text":"Design Provide scenarios for the development team to make the AI Our goal is to make an AI playtest agent that could learn to play a turn-based card game and provide valuable feedback to the game designer. Valuable feedback includes: Game Balance Economy Balance Game Mechanics Game too easy/hard Dominant strategy Card Design Deck Building To deliver that feedback, the very first thing we need to do is make sure that our AI knows how to play the game, and master the game. The first deck that I provided: Strike+ : Deal 9 damage *4 Defend : Gain 5 Block *3 Shrug it off : Gain 8 Block. Draw 1 card. Pommel Strike+ : Deal 10 damage. Draw 2 cards Flex : Gain 2 strength, and lost it at the end of the game Thunderclap : Deal 4 damage and apply 1 vulnerable to all enemies Twin Strike+ : Deal 7 damage twice Bludgeon : Deal 32 damage This deck has a 100% win rate when a human player plays it. This means our AI should have a win rate highly close to 100%. This makes it easier for our development team to see how good our AI is. This deck is: Easy It only has 8 different cards, 1 debuff for the boss, and 1 buff for the player. It only has 13 cards, each card gets recycled very fast. Bludgeon A very useful card that dealt a significant amount of damage Flex 0 cost card that can boost up attack cards. This card should be played first when drawing it, this will be a good reference for us to see our AI\u2019s strategy. We are anticipating our AI will figure out the correct combo of playing Flex Thunderclap and then another attack card. What do we learn from this deck? Actually, this is not the first version of the deck, in the very first version, we have bludgeon+ and bash+ instead of bludgeon and thunderclap. And we had a 100% win rate. The reason we decided to change that is that the player could always win by only playing attack cards. If we use the deck to test our AI, it could have a very high win rate even if our AI is not sophisticated enough. After changing to the current deck, a random bot will not work in this situation. The player needs to manage when to play attack and when to play defense. During the human playtest result, the ending margin of player HP and boss HP are pretty close, close to zero. How does our AI perform on this deck? Our AI finally got nearly 90% of the win rate on this deck, however, it is still not willing to play flex before other attack cards. In the designer\u2019s mind, we thought that always playing flex at the beginning of the turn is an obvious move. The AI is not following that. The second deck that I provided: Bash : Deal 8 damage. Apply 2 Vulnerable. Shockwave : Apply 3 Weak and Vulnerable to ALL enemies. Exhaust. Sword Boomerang+ : Deal 3 damage to a random enemy 4 times. *2 Cleave+ : Deal 11 damage to ALL enemies. *2 Iron Wave : Gain 5 Block. Deal 5 damage. *2 Feed+ : Deal 12 damage. If this kills a non-minion enemy, gain 4 permanent Max HP. Exhaust. Clothesline+ : Deal 14 damage. Apply 3 Weak. Heavy Blade+ : Deal 14 damage. Strength affects Heavy Blade 5 times. *2 Impervious : Gain 30 Block. Exhaust. Disarm : Enemy loses 2 Strength. Exhaust. Double Tap : This turn, your next Attack is played twice. Uppercut : Deal 13 damage. Apply 1 Weak. Apply 1 Vulnerable. According to our playtest, we also secured a 100% win rate on this deck. This deck is a lot harder than the first deck, it involves more exhaust cards. Exhaust card means card that only could play once in the entire boss fight. This deck is: Hard It has 13 different cards, each of them has unique usability. It has more buffs. It only has 2 to 3 defensive cards with less block value. Majority of the cards cost 2 energy. Double Tap A powerful card that empowers the next attack cards. Exhaust cards Can only play once in the entire game. It provides a deeper task to the AI to figure out which turn is the best turn to play the exhaust cards. What do we learn from this deck? When the AI could win the game, it will not try to maximize its damage in every single turn. So the difficulty of a deck is related to the performance of the AI, if the AI faces a much more difficult deck, it will try to maximize its damage. How does our AI perform on this deck? Our AI finally got 87% of the win rate on this deck, which is pretty good. The AI seems more likely to use 2 same attack cards instead of double tap and then attack card. The results are the same.","title":"Design"},{"location":"design/#design","text":"","title":"Design"},{"location":"design/#provide-scenarios-for-the-development-team-to-make-the-ai","text":"Our goal is to make an AI playtest agent that could learn to play a turn-based card game and provide valuable feedback to the game designer. Valuable feedback includes: Game Balance Economy Balance Game Mechanics Game too easy/hard Dominant strategy Card Design Deck Building To deliver that feedback, the very first thing we need to do is make sure that our AI knows how to play the game, and master the game.","title":"Provide scenarios for the development team to make the AI"},{"location":"design/#the-first-deck-that-i-provided","text":"Strike+ : Deal 9 damage *4 Defend : Gain 5 Block *3 Shrug it off : Gain 8 Block. Draw 1 card. Pommel Strike+ : Deal 10 damage. Draw 2 cards Flex : Gain 2 strength, and lost it at the end of the game Thunderclap : Deal 4 damage and apply 1 vulnerable to all enemies Twin Strike+ : Deal 7 damage twice Bludgeon : Deal 32 damage This deck has a 100% win rate when a human player plays it. This means our AI should have a win rate highly close to 100%. This makes it easier for our development team to see how good our AI is. This deck is: Easy It only has 8 different cards, 1 debuff for the boss, and 1 buff for the player. It only has 13 cards, each card gets recycled very fast. Bludgeon A very useful card that dealt a significant amount of damage Flex 0 cost card that can boost up attack cards. This card should be played first when drawing it, this will be a good reference for us to see our AI\u2019s strategy. We are anticipating our AI will figure out the correct combo of playing Flex Thunderclap and then another attack card. What do we learn from this deck? Actually, this is not the first version of the deck, in the very first version, we have bludgeon+ and bash+ instead of bludgeon and thunderclap. And we had a 100% win rate. The reason we decided to change that is that the player could always win by only playing attack cards. If we use the deck to test our AI, it could have a very high win rate even if our AI is not sophisticated enough. After changing to the current deck, a random bot will not work in this situation. The player needs to manage when to play attack and when to play defense. During the human playtest result, the ending margin of player HP and boss HP are pretty close, close to zero. How does our AI perform on this deck? Our AI finally got nearly 90% of the win rate on this deck, however, it is still not willing to play flex before other attack cards. In the designer\u2019s mind, we thought that always playing flex at the beginning of the turn is an obvious move. The AI is not following that.","title":"The first deck that I provided:"},{"location":"design/#the-second-deck-that-i-provided","text":"Bash : Deal 8 damage. Apply 2 Vulnerable. Shockwave : Apply 3 Weak and Vulnerable to ALL enemies. Exhaust. Sword Boomerang+ : Deal 3 damage to a random enemy 4 times. *2 Cleave+ : Deal 11 damage to ALL enemies. *2 Iron Wave : Gain 5 Block. Deal 5 damage. *2 Feed+ : Deal 12 damage. If this kills a non-minion enemy, gain 4 permanent Max HP. Exhaust. Clothesline+ : Deal 14 damage. Apply 3 Weak. Heavy Blade+ : Deal 14 damage. Strength affects Heavy Blade 5 times. *2 Impervious : Gain 30 Block. Exhaust. Disarm : Enemy loses 2 Strength. Exhaust. Double Tap : This turn, your next Attack is played twice. Uppercut : Deal 13 damage. Apply 1 Weak. Apply 1 Vulnerable. According to our playtest, we also secured a 100% win rate on this deck. This deck is a lot harder than the first deck, it involves more exhaust cards. Exhaust card means card that only could play once in the entire boss fight. This deck is: Hard It has 13 different cards, each of them has unique usability. It has more buffs. It only has 2 to 3 defensive cards with less block value. Majority of the cards cost 2 energy. Double Tap A powerful card that empowers the next attack cards. Exhaust cards Can only play once in the entire game. It provides a deeper task to the AI to figure out which turn is the best turn to play the exhaust cards. What do we learn from this deck? When the AI could win the game, it will not try to maximize its damage in every single turn. So the difficulty of a deck is related to the performance of the AI, if the AI faces a much more difficult deck, it will try to maximize its damage. How does our AI perform on this deck? Our AI finally got 87% of the win rate on this deck, which is pretty good. The AI seems more likely to use 2 same attack cards instead of double tap and then attack card. The results are the same.","title":"The second deck that I provided:"},{"location":"generalization/","text":"Generalization","title":"Generalization"},{"location":"generalization/#generalization","text":"","title":"Generalization"},{"location":"guide/","text":"App Instructions Introduction This is an introduction to help walk you through the experience of the AI Playtesting prototype. The game we target is called Slay the Spire, a turn-based combat card game. Card designers (users) can design their decks and cards, then train an AI to playtest their sets of cards. AI will provide statistics like win rate, frequency of each card, etc. to help users balance their cards. Workflow Start the application in 3 steps: Unzip file in the desktop or some folder that doesn\u2019t have a long path. One safe way is to unzip under C:// folder. (Windows has a path length limitation of 256 characters.) Go to the path AIPABuild\\BoardEngine-win32-x64, then open an executable file called BoardEngine. (If a windows security warning pops up, you should select 'More info' and then 'run anyway' to proceed.) Click GO to enter the application. Run the application in 3 steps: 1. Design Play button : Also known as human playtesting. It will open Unity games to play against the boss on your own. After Clicking on the Play button, you can click on State Game to play. Note that if \u201cenable AI\u201d is checked, the reward of each card is calculated and displayed when you are playing the game. Deck config : select different decks that you want to play. Add button can add a new deck into this particular app. Add or delete cards : Add more cards in your deck, or delete cards that you don\u2019t like. 2. Training: Specify the number of iterations, then click on the Train button to train. Small number of iterations take less time to run, but may cause less accuracy rate. On the contrary, larger numbers of iterations take more time, but have a better chance to achieve a higher win rate. After clicking on the Train button, It will take some time, from a couple of minutes to hours, to run the training. After the training is over, we can move on to the third part: playtest. 3. Playtesting : After training, we can run playtest to generate some statistics. Specify number of games to playtest, and select a trained model from the select bar on the third column, and then run playtest. After playtesting is done, we can see statistics results as the graph below. The leftmost pie chart is the win rate that AI played. Average game length means how many turns a game goes on average. Average player/boss hp means when a game ends, how much is the player\u2019s/boss\u2019s hp on average over all games playtested. The rightmost bar chart shows various statistics for advanced designers. Challenges to playtesters Construct a deck that can achieve 100% win rate. Construct a deck that achieves around 50% win rate. That being said, form a balanced level of difficulty for other players. Create a new card and add to a deck. Is it going to improve your win rate or not?","title":"App Instructions"},{"location":"guide/#app-instructions","text":"","title":"App Instructions"},{"location":"guide/#introduction","text":"This is an introduction to help walk you through the experience of the AI Playtesting prototype. The game we target is called Slay the Spire, a turn-based combat card game. Card designers (users) can design their decks and cards, then train an AI to playtest their sets of cards. AI will provide statistics like win rate, frequency of each card, etc. to help users balance their cards.","title":"Introduction"},{"location":"guide/#workflow","text":"Start the application in 3 steps: Unzip file in the desktop or some folder that doesn\u2019t have a long path. One safe way is to unzip under C:// folder. (Windows has a path length limitation of 256 characters.) Go to the path AIPABuild\\BoardEngine-win32-x64, then open an executable file called BoardEngine. (If a windows security warning pops up, you should select 'More info' and then 'run anyway' to proceed.) Click GO to enter the application. Run the application in 3 steps:","title":"Workflow"},{"location":"guide/#1-design","text":"Play button : Also known as human playtesting. It will open Unity games to play against the boss on your own. After Clicking on the Play button, you can click on State Game to play. Note that if \u201cenable AI\u201d is checked, the reward of each card is calculated and displayed when you are playing the game. Deck config : select different decks that you want to play. Add button can add a new deck into this particular app. Add or delete cards : Add more cards in your deck, or delete cards that you don\u2019t like.","title":"1. Design"},{"location":"guide/#2-training","text":"Specify the number of iterations, then click on the Train button to train. Small number of iterations take less time to run, but may cause less accuracy rate. On the contrary, larger numbers of iterations take more time, but have a better chance to achieve a higher win rate. After clicking on the Train button, It will take some time, from a couple of minutes to hours, to run the training. After the training is over, we can move on to the third part: playtest.","title":"2. Training:"},{"location":"guide/#3-playtesting","text":"After training, we can run playtest to generate some statistics. Specify number of games to playtest, and select a trained model from the select bar on the third column, and then run playtest. After playtesting is done, we can see statistics results as the graph below. The leftmost pie chart is the win rate that AI played. Average game length means how many turns a game goes on average. Average player/boss hp means when a game ends, how much is the player\u2019s/boss\u2019s hp on average over all games playtested. The rightmost bar chart shows various statistics for advanced designers.","title":"3. Playtesting:"},{"location":"guide/#challenges-to-playtesters","text":"Construct a deck that can achieve 100% win rate. Construct a deck that achieves around 50% win rate. That being said, form a balanced level of difficulty for other players. Create a new card and add to a deck. Is it going to improve your win rate or not?","title":"Challenges to playtesters"},{"location":"introduction/","text":"Introduction Video (8 minutes)","title":"Introduction"},{"location":"introduction/#introduction-video-8-minutes","text":"","title":"Introduction Video (8 minutes)"},{"location":"page1/","text":"This is page 1","title":"This is page 1"},{"location":"page1/#this-is-page-1","text":"","title":"This is page 1"},{"location":"page2/","text":"page22222 header","title":"Page2"},{"location":"page2/#page22222-header","text":"","title":"page22222 header"},{"location":"takeaways/","text":"Takeaways","title":"Takeaways"},{"location":"takeaways/#takeaways","text":"","title":"Takeaways"},{"location":"blogs/week1/","text":"Week 1 : Getting Started The objective of our project is to create an AI that can playtest card games and help balance it. We expect that the AI will identify dominant strategies to win the game, and this can lead us to making game design updates to the game. To do this, we are going to design our own card game because we want flexibility over the rules of the game. We want to test our AI with different game mechanics to see what problems (in the context of card game mechanics) it can solve easily and where it struggles. We plan to use Python to build our game kernel and have some communication with Unity for visualization of game play. For our project to work successful, we have divided it up into multiple aspects \u2013 the core game kernel, the AI, communication with Unity and game design for the card game. We need to work towards implementing each of these and ensure that they work well together. Game-AI-Input System The following is a simple representation of our system that implements the above. Here are the requirements from this system: All the gameplay logic executed in a module called\u201dgameplayKernel\u201d Other modules are only able to provide userInput and get what gameState and triggered gameEvents after this input. Exactly how game logic is executed is 100% invisible to them. All the gamedata is separated into a structure call game state. Game kernel is 100% decoupled with graphics/UI and AI training In the first week, our objective was to come up with a playable Rock-Paper-Scissor-Dragon card game and then train an AI agent to play it. RPSD is our take on the classic (and balanced) Rock-Paper-Scissor game with the exception that there is a Dragon action that defeats rock, paper and scissors but draws to another Dragon. The idea behind doing this is to prove that in the simplest of settings, the AI agent can identify the obviously broken part of the game. Unity \u2013 Python Connection Research on the ways to achieve this: To have a python interpreter in .NET environment (https://ironpython.net/) . The cons is this is not stable, and will have some trouble when using external python packages like numpy. Use command line to execute the python program, and redirect the standard input/output from cmd to C#/Unity environment. This is doable, but using standard input/output to communicate is inconvenience. And this need player to install the python environment Python programs run in another process, and Unity in another process. Use socket to transfer the information between to language. This is what we finally chose, but it still needs more work. We need to think about how to call a function and share object instances. AI Agent Since the AI problem is so simple, we implemented followed a simple tabular reinforcement learning approach. To elaborate, we maintain a table of the expected reward from taking each of the four actions (rock, paper, scissor or dragon). We update these expected reward values based on the rewards from each game. The agent uses an epsilon-greedy approach with a constant epsilon value of 0.1. Since the dragon will naturally has the highest chance of winning any game, over a period of time the expected reward for a dragon is the highest. Here are some charts of the training results. Each plot shows the number of times the AI agent picked each action. We see that after 100,000 steps, the AI agent predominantly picks the dragon action. The minority of actions that are not dragon can be attributed to the epsilon probability value. Although, this worked, we are well aware that this problem is trivial compared to building an AI to play a complex card game. Since the action and state space can be very large, a tabular reinforcement learning approach can no longer work. In this situation, we need to use a neural network to estimate expected reward. The input layer can be a vector of integer values that represent the current state space. The output layer is more tricky because it involves a fairly complex action selection. Game Design Last but not the least, we arrive at the game design component of our project. We plan to have a game that starts with a simple rules and few mechanics but eventually evolves into something more complex. This can help with us progressively increase the reinforcement learning model complexity. The first version of the game only involves two systems: Hero system Card system The hero system consists of two set of actions to choose from. Player has to decide the positioning of each hero in each round, because normally a hero can only attack the enemy that is in front of it. Additionally, each hero will have two extra skills that require a particular resource to unlock, which is the second set of actions that needs decision making. The card system also requires players to make decisions between short long term rewards. The player may choose to deal maximum damage each turn, or choose to gain resources to level up and fight back. Ideally, we want a variety of different strategies to be viable The game, of course, requires large amounts of balancing. For example, the HP and attack damage of each hero, the amount of resources that are required to level up a skill, and the drawing probabilities of each card are all crucial to this game being balanced. This is where we hope the AI can help us out. In conclusion, we believe we have had a strong start to the project. Our objectives that we need to achieve are clear. We want to explore relatively unexplored territory that is technically very challenging. All we know for certain is that we are excited!","title":"Week 1 : Getting Started"},{"location":"blogs/week1/#week-1-getting-started","text":"The objective of our project is to create an AI that can playtest card games and help balance it. We expect that the AI will identify dominant strategies to win the game, and this can lead us to making game design updates to the game. To do this, we are going to design our own card game because we want flexibility over the rules of the game. We want to test our AI with different game mechanics to see what problems (in the context of card game mechanics) it can solve easily and where it struggles. We plan to use Python to build our game kernel and have some communication with Unity for visualization of game play. For our project to work successful, we have divided it up into multiple aspects \u2013 the core game kernel, the AI, communication with Unity and game design for the card game. We need to work towards implementing each of these and ensure that they work well together.","title":"Week 1 : Getting Started"},{"location":"blogs/week1/#game-ai-input-system","text":"The following is a simple representation of our system that implements the above. Here are the requirements from this system: All the gameplay logic executed in a module called\u201dgameplayKernel\u201d Other modules are only able to provide userInput and get what gameState and triggered gameEvents after this input. Exactly how game logic is executed is 100% invisible to them. All the gamedata is separated into a structure call game state. Game kernel is 100% decoupled with graphics/UI and AI training In the first week, our objective was to come up with a playable Rock-Paper-Scissor-Dragon card game and then train an AI agent to play it. RPSD is our take on the classic (and balanced) Rock-Paper-Scissor game with the exception that there is a Dragon action that defeats rock, paper and scissors but draws to another Dragon. The idea behind doing this is to prove that in the simplest of settings, the AI agent can identify the obviously broken part of the game.","title":"Game-AI-Input System"},{"location":"blogs/week1/#unity-python-connection","text":"Research on the ways to achieve this: To have a python interpreter in .NET environment (https://ironpython.net/) . The cons is this is not stable, and will have some trouble when using external python packages like numpy. Use command line to execute the python program, and redirect the standard input/output from cmd to C#/Unity environment. This is doable, but using standard input/output to communicate is inconvenience. And this need player to install the python environment Python programs run in another process, and Unity in another process. Use socket to transfer the information between to language. This is what we finally chose, but it still needs more work. We need to think about how to call a function and share object instances.","title":"Unity \u2013 Python Connection"},{"location":"blogs/week1/#ai-agent","text":"Since the AI problem is so simple, we implemented followed a simple tabular reinforcement learning approach. To elaborate, we maintain a table of the expected reward from taking each of the four actions (rock, paper, scissor or dragon). We update these expected reward values based on the rewards from each game. The agent uses an epsilon-greedy approach with a constant epsilon value of 0.1. Since the dragon will naturally has the highest chance of winning any game, over a period of time the expected reward for a dragon is the highest. Here are some charts of the training results. Each plot shows the number of times the AI agent picked each action. We see that after 100,000 steps, the AI agent predominantly picks the dragon action. The minority of actions that are not dragon can be attributed to the epsilon probability value. Although, this worked, we are well aware that this problem is trivial compared to building an AI to play a complex card game. Since the action and state space can be very large, a tabular reinforcement learning approach can no longer work. In this situation, we need to use a neural network to estimate expected reward. The input layer can be a vector of integer values that represent the current state space. The output layer is more tricky because it involves a fairly complex action selection.","title":"AI Agent"},{"location":"blogs/week1/#game-design","text":"Last but not the least, we arrive at the game design component of our project. We plan to have a game that starts with a simple rules and few mechanics but eventually evolves into something more complex. This can help with us progressively increase the reinforcement learning model complexity. The first version of the game only involves two systems: Hero system Card system The hero system consists of two set of actions to choose from. Player has to decide the positioning of each hero in each round, because normally a hero can only attack the enemy that is in front of it. Additionally, each hero will have two extra skills that require a particular resource to unlock, which is the second set of actions that needs decision making. The card system also requires players to make decisions between short long term rewards. The player may choose to deal maximum damage each turn, or choose to gain resources to level up and fight back. Ideally, we want a variety of different strategies to be viable The game, of course, requires large amounts of balancing. For example, the HP and attack damage of each hero, the amount of resources that are required to level up a skill, and the drawing probabilities of each card are all crucial to this game being balanced. This is where we hope the AI can help us out. In conclusion, we believe we have had a strong start to the project. Our objectives that we need to achieve are clear. We want to explore relatively unexplored territory that is technically very challenging. All we know for certain is that we are excited!","title":"Game Design"},{"location":"blogs/week10/","text":"Week 10 : Preparing For Playtesting This week was all about bringing together our app so that it can be used for playtesting. There was a significant progress on this front with multiple different windows in the app being added. There was some minor progress on the AI front which bumped the win rate up to a stable ~82% and a peak of 88%. This brings us closer to our goal of 90%. However, experiments with the AI have slowed down because a lot of our efforts are not focused on completing the app for playtesting. AI Updates There were three areas where the AI made progress this week. Let\u2019s get into each one by one: Reward Function In another update to the reward function, we decided to use a peculiar kind of reward function. Up until now, we were using a Monte-Carlo estimation of reward for a state action pair. This meant that we take the final reward of the trajectory at the final step, then go back one step at a time and assign a reward (discounted by the number of steps from itself to the final step) to each state-action in the trajectory. This week, the peculiar thing that we did was to first calculate this reward for each state-action in the trajectory and then use this to create something similar to a temporal difference target. This is achieved by going up the trajectory starting at the current state and replacing the current state-actions\u2019s reward by an addition of itself and the next state-action\u2019s reward. This seems like a weird thing to do but it did end up giving us a little bit of a better result than what we have received earlier. Reducing Model Complexity As a part of the push we are making to go towards creating an application for designers to use, we want to reduce the time it takes to train an AI model. One of the ways this can be done is by modifying the hyperparameters. For one, we reduced the model complexity by reducing the number of layers and also reduced the number of neurons on each layer. Another thing was to reduce the batch size during training. Earlier the batch size was set to a much higher value than required (512). Reducing the batch size delivered the same approximate win rate. A potential benefit of this could be that we are avoiding overfitting. Integration with the App The last important piece of progress this week was to complete the module that allows training by clicking a button on the app. You can also watch the progress of training as well as watch as the progress of the app improves. Currently, this is the only feature in place but we are working on creating test data using the generated model and then visualizing that data in the app. Build Across Multiple Frameworks This week, we successfully packaged everything; unity, python, tensorflow and electron into one executable. This allows us now to distribute our application easily to playtesters. Main components of build When we work on Unity, Unity itself helps to build the game in Unity. When working on electron, we use electron-forge to build everything in the electron. However, our application uses both unity, electron and python. Unfortunately, there are no official tools to build them together. Electron native build: We use electron-forge to build resources managed by electron. The main difference between normal electron build and ours is that there are many resources not managed by electron, such as unity code and python code. Unity executables : There are two Unity executables. One is a GUI for playing the game, and the other is the record playback tool. In an ideal situation, these 2 should be combined into one app. Since unity builds are very small, two builds are acceptable in size. Python build: We discussed how to build python into unity in a previous blog post ( Week 6 ). We analyzed the pros and cons of different approaches. We yet again decided use \u201cpython interpreter + source code\u201d for the same reason Database: These are all static files. These need to be copied in the correct directory. Several tries in python build In the past, we said we built our unity app using the approach \u201cpython interpreter + source code\u201d. But we actually only did half of it, the source code. We didn\u2019t consider the python interpreter, in other words, the python environment. Providing an installer in the electron app We put a python installer into the build, and provide the GUI for user to install python and tensorflow. This didn\u2019t work well when we tested on a teammate\u2019s computers. One of the problems is that there are many installation configurations, such as version, path,etc. Unknown problems can happen if users don\u2019t configure it in the right way. An additional problem is that some teammates already have installed the python in their computer, but with different version and environment. Prebuilt python environment into electron app (Final choice) To make the python environment more stable and convenient, we preinstall the whole python environment, including interpreter and dependencies, into the final executable. Even though the final build grew from 250MB to 1.6GB, the build is stable and under control. This is very important for us because our app is built across so many frameworks. Insights and Problems BUILD CROSS DIFFERENT FRAMEWORKS/PLATFORMS Our project is an example of multiple frameworks and platforms. When we want to distribute our project by binaries, we consider two stages: Stage1: Prebuild First stage is to build all modules in their native platforms, as we discussed in Unity, electron, and python. In our case, our biggest challenge is to build all of the python code. Stage2: Combination and Communication When we say combine the build, it\u2019s mainly about how to manage the paths, how to read files in the build files. Almost all frameworks (in our case, Unity and Electron) have their own way to get dynamic relative path during runtime, so this is not a problem. As for the communication, because we use socket to do the inter-process communication, as long as we use the right port, the operating system will handle the rest. Develop mode V.S. build runtime Most frameworks have separate code for development and build. We have the same situation, because the way we find code in other platforms is different between develop mode and build runtime. Now we need to manually redirect the code (latest development code or code in last version\u2019s build) during development. This is very inconvenient and can potentially cause a lot of bugs. Our solution in the coming week, is to automate the multi-platform build procedure so that we manage the relationship between development and build. Manual operation is the devil, we need to wipe this out of our build procedure! Data Visualization This week we learnt how to use d3.js to build interactive and professional data visualizations. We can use it to develop the different kinds of data visualization we discussed last week. The data we currently use (in the pictures below) is dummy data. However, in the next week, we will get real data from AI Playtesting. All in all, this week saw some much needed progress on the app. We are excited to see how playtesters respond during the playtesting session on 10-11 Nov.","title":"Week 10 : Preparing For Playtesting"},{"location":"blogs/week10/#week-10-preparing-for-playtesting","text":"This week was all about bringing together our app so that it can be used for playtesting. There was a significant progress on this front with multiple different windows in the app being added. There was some minor progress on the AI front which bumped the win rate up to a stable ~82% and a peak of 88%. This brings us closer to our goal of 90%. However, experiments with the AI have slowed down because a lot of our efforts are not focused on completing the app for playtesting.","title":"Week 10 : Preparing For Playtesting"},{"location":"blogs/week10/#ai-updates","text":"There were three areas where the AI made progress this week. Let\u2019s get into each one by one:","title":"AI Updates"},{"location":"blogs/week10/#reward-function","text":"In another update to the reward function, we decided to use a peculiar kind of reward function. Up until now, we were using a Monte-Carlo estimation of reward for a state action pair. This meant that we take the final reward of the trajectory at the final step, then go back one step at a time and assign a reward (discounted by the number of steps from itself to the final step) to each state-action in the trajectory. This week, the peculiar thing that we did was to first calculate this reward for each state-action in the trajectory and then use this to create something similar to a temporal difference target. This is achieved by going up the trajectory starting at the current state and replacing the current state-actions\u2019s reward by an addition of itself and the next state-action\u2019s reward. This seems like a weird thing to do but it did end up giving us a little bit of a better result than what we have received earlier.","title":"Reward Function"},{"location":"blogs/week10/#reducing-model-complexity","text":"As a part of the push we are making to go towards creating an application for designers to use, we want to reduce the time it takes to train an AI model. One of the ways this can be done is by modifying the hyperparameters. For one, we reduced the model complexity by reducing the number of layers and also reduced the number of neurons on each layer. Another thing was to reduce the batch size during training. Earlier the batch size was set to a much higher value than required (512). Reducing the batch size delivered the same approximate win rate. A potential benefit of this could be that we are avoiding overfitting.","title":"Reducing Model Complexity"},{"location":"blogs/week10/#integration-with-the-app","text":"The last important piece of progress this week was to complete the module that allows training by clicking a button on the app. You can also watch the progress of training as well as watch as the progress of the app improves. Currently, this is the only feature in place but we are working on creating test data using the generated model and then visualizing that data in the app.","title":"Integration with the App"},{"location":"blogs/week10/#build-across-multiple-frameworks","text":"This week, we successfully packaged everything; unity, python, tensorflow and electron into one executable. This allows us now to distribute our application easily to playtesters.","title":"Build Across Multiple Frameworks"},{"location":"blogs/week10/#main-components-of-build","text":"When we work on Unity, Unity itself helps to build the game in Unity. When working on electron, we use electron-forge to build everything in the electron. However, our application uses both unity, electron and python. Unfortunately, there are no official tools to build them together. Electron native build: We use electron-forge to build resources managed by electron. The main difference between normal electron build and ours is that there are many resources not managed by electron, such as unity code and python code. Unity executables : There are two Unity executables. One is a GUI for playing the game, and the other is the record playback tool. In an ideal situation, these 2 should be combined into one app. Since unity builds are very small, two builds are acceptable in size. Python build: We discussed how to build python into unity in a previous blog post ( Week 6 ). We analyzed the pros and cons of different approaches. We yet again decided use \u201cpython interpreter + source code\u201d for the same reason Database: These are all static files. These need to be copied in the correct directory.","title":"Main components of build"},{"location":"blogs/week10/#several-tries-in-python-build","text":"In the past, we said we built our unity app using the approach \u201cpython interpreter + source code\u201d. But we actually only did half of it, the source code. We didn\u2019t consider the python interpreter, in other words, the python environment. Providing an installer in the electron app We put a python installer into the build, and provide the GUI for user to install python and tensorflow. This didn\u2019t work well when we tested on a teammate\u2019s computers. One of the problems is that there are many installation configurations, such as version, path,etc. Unknown problems can happen if users don\u2019t configure it in the right way. An additional problem is that some teammates already have installed the python in their computer, but with different version and environment.","title":"Several tries in python build"},{"location":"blogs/week10/#prebuilt-python-environment-into-electron-app-final-choice","text":"To make the python environment more stable and convenient, we preinstall the whole python environment, including interpreter and dependencies, into the final executable. Even though the final build grew from 250MB to 1.6GB, the build is stable and under control. This is very important for us because our app is built across so many frameworks.","title":"Prebuilt python environment into electron app (Final choice)"},{"location":"blogs/week10/#insights-and-problems","text":"","title":"Insights and Problems"},{"location":"blogs/week10/#build-cross-different-frameworksplatforms","text":"Our project is an example of multiple frameworks and platforms. When we want to distribute our project by binaries, we consider two stages: Stage1: Prebuild First stage is to build all modules in their native platforms, as we discussed in Unity, electron, and python. In our case, our biggest challenge is to build all of the python code. Stage2: Combination and Communication When we say combine the build, it\u2019s mainly about how to manage the paths, how to read files in the build files. Almost all frameworks (in our case, Unity and Electron) have their own way to get dynamic relative path during runtime, so this is not a problem. As for the communication, because we use socket to do the inter-process communication, as long as we use the right port, the operating system will handle the rest.","title":"BUILD CROSS DIFFERENT FRAMEWORKS/PLATFORMS"},{"location":"blogs/week10/#develop-mode-vs-build-runtime","text":"Most frameworks have separate code for development and build. We have the same situation, because the way we find code in other platforms is different between develop mode and build runtime. Now we need to manually redirect the code (latest development code or code in last version\u2019s build) during development. This is very inconvenient and can potentially cause a lot of bugs. Our solution in the coming week, is to automate the multi-platform build procedure so that we manage the relationship between development and build. Manual operation is the devil, we need to wipe this out of our build procedure!","title":"Develop mode V.S. build runtime"},{"location":"blogs/week10/#data-visualization","text":"This week we learnt how to use d3.js to build interactive and professional data visualizations. We can use it to develop the different kinds of data visualization we discussed last week. The data we currently use (in the pictures below) is dummy data. However, in the next week, we will get real data from AI Playtesting. All in all, this week saw some much needed progress on the app. We are excited to see how playtesters respond during the playtesting session on 10-11 Nov.","title":"Data Visualization"},{"location":"blogs/week11/","text":"Week 11 : All About Integration This week was focused all on integrating the AI into the app and making sure that the one-click-training works correctly. We also focused on creating a new version of the app with reduced complexity as a result of faculty feedback we received indicating that our app can feel a little overwhelming as a result of the complexity. Lastly, we also focused on some UI design for the data visualization. Testing Using AI The main idea behind the project is to facilitate playtesting. In past weak, we reached a stage with the AI where it is performing quite well and it can be integrated into the app as a playtesting entity. This week our goal was to finalize the intergration to the point where the build can be sent our to others for testing. The flow for getting this done involves 3 stages: Creating a Deck : The first step is to create a deck using the available cards (you can create your own cards as well). This is where the game designer\u2019s expertise comes in. A deck can be very strong or weak depending on the cards it has. Training the AI : Once the deck has been created, an AI agents needs to be trained using Reinforcement Learning. This process takes significant time. The recommended number of iterations to train for to get stable winrate is around 10,000-15,000. The improvements after 15,000 iterations are marginal because the model gets stuck asymptotically. 15,000 iterations may take anywhere between 7 to 8 hours. Playtesting with the Trained AI : Once the AI has been trained, you can playtest with it. To generate sufficiently large playtesting data, you need to allow the AI to play for 2000 games or more. This may take somewhere around 10 minutes. The final result of playtesting are the graphs and metrics that are visualized for ease of interpretation. We will get into the details regarding that in the following sections of this blog post. Depending on how strong or weak your deck was, the AI will come up with data reflecting the same. New Version of the App This week, we worked on redesigning the entire app to make it easier to use and understand. The goal is to keep the functions comprehensible for new users. Workflow of the \u2018Basic Mode\u2019 We segregated features of the app into basic mode and advanced mode. In basic mode, there is a very straightforward workflow: design, train and playtest. A picture of how this looks is below: Features in the Design Section of the Basic Mode In the design part of the basic mode, you can: Switch between current decks Add/remove cards from a deck Create a new deck Play your designed game in Unity An image of this is shown below: Features in the Train Section of the Basic Mode We allow the user to select how many iterations to train for. Then just by simply clicking a button, it will connect to our AI module to train the AI on the currently selected deck/game. This can happen because all our code about AI and gameplay are designed as generalized frameworks in the first place! An image of the training section is shown below: Features in the Test Section of the Basic Mode Playtesting is as easy as training the AI agent and is much faster. You can select a trained AI agent and playtest using it to quickly generate data and visualize it. The following is a picture of our current test section. Practice with the Web GUI We implemented the whole basic mode within a single week. By using the web techniques available in electron, we can change the layout and functionality of the UI efficiently. Compared to UI systems we worked on before(such as Unity\u2019s UI), we find the combination of css, bootstrap and jquery is very powerful in our development process. How does this help us? Responsive UI : HTML has very good support for responsive UI in different sizes and resolutions. With the grid system in Bootstrap, our UI always looks good no matter how the window size is modified. Flexible UI : HTML has the concept of DOM; it organizes UI elements in a tree-like structure. This allows us to modify dynamic UI elements and change UI layout efficiently Stylized : CSS has a very decoupled and flexible way to change style. Built-In interactions : Bootstrap has many built-in interactions, such as modals, sliders, collapse, dropdown, etc, which cover most of our needs. Disadvantages Hard to build UI out of the framework : These techniques are very hard to use if the goal is to create UI which cannot be described in existing elements, such as buttons, divs, etc. For example, when we try to render the card with dynamic information from the database, we need to manually tune a lot of the CSS properties. Pool animation support : Compared to game engines like Unity, web techniques have a bare-bone animation support. For example, even a simple round progress bar shown below takes a lot of work because of the poor key-frame mechanism. Infrastructure of our Build As mentioned last week, we don\u2019t have good infrastructure to support our multi-platforms build process. This week, we have made it organized and easy to configure. Problems we were facing Manual operations : Every time we build our app, there are some manual configurations and operations. While we make builds frequently, this is not only inefficient, but also causes many bugs. Messed up Build and Development code : A lot of behavior and code is different, especially when it comes to file paths. Sometimes in order to simulate how it works in the build, we switch the code back and forth between the latest code and those in previous builds. Solution Automate the build procedure : We put all build relevant information in one file like config.json. Then we wrote some scripts to create the final build based on these configurations. Divide project into build mode and develop mode : We encapsulated all operations having differences in development and build modes. Then we can switch between different modes with the configuration file very easily. Challenges with the Build An issue with the build we found this week was with the Windows character limit for file paths . Tensorflow creates a long file names for some of its files that store variables and it can cause some unexpected errors because of the windows limit being violated. It took us a long time to figure out what exactly was causing the error because nothing explicitly pointed to a problem with long file paths. Once we did figure this out, we reduced the lengths of the folders that store these tensorflow models as much as possible. However long file paths are still an issue for us and we are yet to think about a good way to ensure this does not happen in the future. Card and Deck UI in the Advanced Mode The more complex features of our app have been bundled up into the \u2018Advanced Mode\u2019 which is a more complex counterpart of the Basic Mode. This week also saw some major redesigning of the Advanced Mode. We restructured the UI in advanced mode by using Bootstrap, mainly in the card edit page and the card list page. The left side is a navigation bar to make users navigate through other pages. We made the card editing functionality by clicking on the card images to make it more intuitive. Below are some pictures of the redesigned UI: Playtest Plan: Video and Survey We planned to launch our playtest video and survey this week. Due to multiple challenges that we face on development, we failed to create our build on time. We believe it is better to leave this part for now, and come back to playtest after we finish most of the app functionality. Some materials may be used to create our trailer video/project intro video. Data Visualization Mockups Below are some screenshots of some mockups of how our data visualization (generated in the testing stage) should ideally be presented. Basic Playtesting Data: Average game length : shows mean, mode, max and min. Card Utilization : Broken down by different game periods; First quarter, second quarter, third quarter and the last quarter. Card Play Position All in all, this week everyone was involved in working on the app. The design changes makes the app look much more accessible. We want to ensure that we place it in the hands of designers to see where they take it.","title":"Week 11 : All About Integration"},{"location":"blogs/week11/#week-11-all-about-integration","text":"This week was focused all on integrating the AI into the app and making sure that the one-click-training works correctly. We also focused on creating a new version of the app with reduced complexity as a result of faculty feedback we received indicating that our app can feel a little overwhelming as a result of the complexity. Lastly, we also focused on some UI design for the data visualization.","title":"Week 11 : All About Integration"},{"location":"blogs/week11/#testing-using-ai","text":"The main idea behind the project is to facilitate playtesting. In past weak, we reached a stage with the AI where it is performing quite well and it can be integrated into the app as a playtesting entity. This week our goal was to finalize the intergration to the point where the build can be sent our to others for testing. The flow for getting this done involves 3 stages: Creating a Deck : The first step is to create a deck using the available cards (you can create your own cards as well). This is where the game designer\u2019s expertise comes in. A deck can be very strong or weak depending on the cards it has. Training the AI : Once the deck has been created, an AI agents needs to be trained using Reinforcement Learning. This process takes significant time. The recommended number of iterations to train for to get stable winrate is around 10,000-15,000. The improvements after 15,000 iterations are marginal because the model gets stuck asymptotically. 15,000 iterations may take anywhere between 7 to 8 hours. Playtesting with the Trained AI : Once the AI has been trained, you can playtest with it. To generate sufficiently large playtesting data, you need to allow the AI to play for 2000 games or more. This may take somewhere around 10 minutes. The final result of playtesting are the graphs and metrics that are visualized for ease of interpretation. We will get into the details regarding that in the following sections of this blog post. Depending on how strong or weak your deck was, the AI will come up with data reflecting the same.","title":"Testing Using AI"},{"location":"blogs/week11/#new-version-of-the-app","text":"This week, we worked on redesigning the entire app to make it easier to use and understand. The goal is to keep the functions comprehensible for new users.","title":"New Version of the App"},{"location":"blogs/week11/#workflow-of-the-basic-mode","text":"We segregated features of the app into basic mode and advanced mode. In basic mode, there is a very straightforward workflow: design, train and playtest. A picture of how this looks is below:","title":"Workflow of the \u2018Basic Mode\u2019"},{"location":"blogs/week11/#features-in-the-design-section-of-the-basic-mode","text":"In the design part of the basic mode, you can: Switch between current decks Add/remove cards from a deck Create a new deck Play your designed game in Unity An image of this is shown below:","title":"Features in the Design Section of the Basic Mode"},{"location":"blogs/week11/#features-in-the-train-section-of-the-basic-mode","text":"We allow the user to select how many iterations to train for. Then just by simply clicking a button, it will connect to our AI module to train the AI on the currently selected deck/game. This can happen because all our code about AI and gameplay are designed as generalized frameworks in the first place! An image of the training section is shown below:","title":"Features in the Train Section of the Basic Mode"},{"location":"blogs/week11/#features-in-the-test-section-of-the-basic-mode","text":"Playtesting is as easy as training the AI agent and is much faster. You can select a trained AI agent and playtest using it to quickly generate data and visualize it. The following is a picture of our current test section.","title":"Features in the Test Section of the Basic Mode"},{"location":"blogs/week11/#practice-with-the-web-gui","text":"We implemented the whole basic mode within a single week. By using the web techniques available in electron, we can change the layout and functionality of the UI efficiently. Compared to UI systems we worked on before(such as Unity\u2019s UI), we find the combination of css, bootstrap and jquery is very powerful in our development process.","title":"Practice with the Web GUI"},{"location":"blogs/week11/#how-does-this-help-us","text":"Responsive UI : HTML has very good support for responsive UI in different sizes and resolutions. With the grid system in Bootstrap, our UI always looks good no matter how the window size is modified. Flexible UI : HTML has the concept of DOM; it organizes UI elements in a tree-like structure. This allows us to modify dynamic UI elements and change UI layout efficiently Stylized : CSS has a very decoupled and flexible way to change style. Built-In interactions : Bootstrap has many built-in interactions, such as modals, sliders, collapse, dropdown, etc, which cover most of our needs.","title":"How does this help us?"},{"location":"blogs/week11/#disadvantages","text":"Hard to build UI out of the framework : These techniques are very hard to use if the goal is to create UI which cannot be described in existing elements, such as buttons, divs, etc. For example, when we try to render the card with dynamic information from the database, we need to manually tune a lot of the CSS properties. Pool animation support : Compared to game engines like Unity, web techniques have a bare-bone animation support. For example, even a simple round progress bar shown below takes a lot of work because of the poor key-frame mechanism.","title":"Disadvantages"},{"location":"blogs/week11/#infrastructure-of-our-build","text":"As mentioned last week, we don\u2019t have good infrastructure to support our multi-platforms build process. This week, we have made it organized and easy to configure.","title":"Infrastructure of our Build"},{"location":"blogs/week11/#problems-we-were-facing","text":"Manual operations : Every time we build our app, there are some manual configurations and operations. While we make builds frequently, this is not only inefficient, but also causes many bugs. Messed up Build and Development code : A lot of behavior and code is different, especially when it comes to file paths. Sometimes in order to simulate how it works in the build, we switch the code back and forth between the latest code and those in previous builds.","title":"Problems we were facing"},{"location":"blogs/week11/#solution","text":"Automate the build procedure : We put all build relevant information in one file like config.json. Then we wrote some scripts to create the final build based on these configurations. Divide project into build mode and develop mode : We encapsulated all operations having differences in development and build modes. Then we can switch between different modes with the configuration file very easily.","title":"Solution"},{"location":"blogs/week11/#challenges-with-the-build","text":"An issue with the build we found this week was with the Windows character limit for file paths . Tensorflow creates a long file names for some of its files that store variables and it can cause some unexpected errors because of the windows limit being violated. It took us a long time to figure out what exactly was causing the error because nothing explicitly pointed to a problem with long file paths. Once we did figure this out, we reduced the lengths of the folders that store these tensorflow models as much as possible. However long file paths are still an issue for us and we are yet to think about a good way to ensure this does not happen in the future.","title":"Challenges with the Build"},{"location":"blogs/week11/#card-and-deck-ui-in-the-advanced-mode","text":"The more complex features of our app have been bundled up into the \u2018Advanced Mode\u2019 which is a more complex counterpart of the Basic Mode. This week also saw some major redesigning of the Advanced Mode. We restructured the UI in advanced mode by using Bootstrap, mainly in the card edit page and the card list page. The left side is a navigation bar to make users navigate through other pages. We made the card editing functionality by clicking on the card images to make it more intuitive. Below are some pictures of the redesigned UI:","title":"Card and Deck UI in the Advanced Mode"},{"location":"blogs/week11/#playtest-plan-video-and-survey","text":"We planned to launch our playtest video and survey this week. Due to multiple challenges that we face on development, we failed to create our build on time. We believe it is better to leave this part for now, and come back to playtest after we finish most of the app functionality. Some materials may be used to create our trailer video/project intro video.","title":"Playtest Plan: Video and Survey"},{"location":"blogs/week11/#data-visualization-mockups","text":"Below are some screenshots of some mockups of how our data visualization (generated in the testing stage) should ideally be presented. Basic Playtesting Data: Average game length : shows mean, mode, max and min. Card Utilization : Broken down by different game periods; First quarter, second quarter, third quarter and the last quarter. Card Play Position All in all, this week everyone was involved in working on the app. The design changes makes the app look much more accessible. We want to ensure that we place it in the hands of designers to see where they take it.","title":"Data Visualization Mockups"},{"location":"blogs/week12/","text":"","title":"Week12"},{"location":"blogs/week13/","text":"","title":"Week13"},{"location":"blogs/week14/","text":"","title":"Week14"},{"location":"blogs/week15/","text":"","title":"Week15"},{"location":"blogs/week2/","text":"Week 2 : First Version Our goal for the week was to start building the first version of our card game. The components of our system architecture include the game kernel, the AI agent and visualization of the game states in Unity. The other important aspect is the design of the card game which is really what everything is for. Card Game Design As discussed in the last week\u2019s post, our strategy is to start with a simple card game and progressively introduce more complexity. This week, we came up with the first iteration of our game and here are the rules. Each player will have the same health amount and attack damage. Only two kinds of cards inside the deck: Heal and Damage. First round each player will have 5 cards and will draw 2 cards each round from the second round. Players will take turns, and they will use all of their cards in hand. Game Kernel The following is a list of files that we have implemented and what each of them does: GameSimulator.py: Allows us to simulate the game multiple times and view results. GameplayKernel.py: Kernel class that instantiates a game state object to \u2018play\u2019 the card game. commonDef.py: Defines game state related classes in order to have a concise interface for Unity and AI agent training. AutoPlayerDef.py: A bot that randomly picks one card from its hand to play out in each round. constantDef.py: Defines constant values used in our project. AI Agent Since we cannot use a tabular approach for our game (as the state-action space is very large), we need to use a neural network. The input layer to the neural network will pass in state defining variable values. We still working on a comprehensive list of such variables. The output layer will then give us a probability distribution over the action space. Using this, we can sample from an action from it or simply take the action that has the highest probability (we plan to try both). The design is of the Policy Gradient RL method. The algorithm that we will begin with is the Advantage Actor Critic. The reason for beginning with policy based methods (over value based methods) is that they perform better in stochastic environments. This applies to our project, because the same actions can result in a different future states based on the moves made by the opponent. During the course of our project, it is likely that we shall try several different approaches to train our AI. We are open to trying value based methods if the Advantage Actor Critic algorithm does not perform well in our setting. Another important aspect to consider is the number of states that we pass into the input layer during training. Since playing a particular card could lead to victory multiple turns later, the reward received after taking a particular action must include discounted future rewards as well. This means that we follow a N-step reward where N is the number of future steps to consider for the reward. Visualization and Input Handling in Unity There are several parts to the Unity system that we are building alongside with our Python game. Networking System For playtesting reasons, we want to allow people to play this game remotely. To that end, we are building a networked Unity system that can allow people to provide inputs remotely through the client. We are using Photon to do this. The reasons for using Photon: Provides free server usage within a limit. Provides easy-to-use API for building room/lobby-based game service. Great support and tutorial resources in Unity. Using Photon, we have built a basic matching system that works for us. It currently includes functionality for: Connecting players to the service. Manage the connection and disconnection. Logic of \u2018Ready\u2019, \u2018Start Game\u2019, and player chat. Below are some screenshots of how this looks currently. Rules of communication between Python and Unity (C#) We have discussed the rules of how AI, Unity logic, python gameplayer will communicate (to ensure less work when we connect everything). Definition and format of important classes such as gamestate/user input is predefined and stored in a python file (commonDef.py). C#/Python communication: Only C# accesses Python, not the other way around C# can only do the following: Call functions, share data structures, and get the snapshot of object instances in python. C# and Python will share the definition of classes which are commonly referenced by both. Const values and settings will be stored in separate files(XML, JSON, etc.) and will be dynamically loaded when required. C# Coding Standard We added a coding style guide of Unity/C# in our Github repository\u2019s readme file, including rules for class layout, nomenclature, declaration and brace style.","title":"Week 2 : First Version"},{"location":"blogs/week2/#week-2-first-version","text":"Our goal for the week was to start building the first version of our card game. The components of our system architecture include the game kernel, the AI agent and visualization of the game states in Unity. The other important aspect is the design of the card game which is really what everything is for.","title":"Week 2 : First Version"},{"location":"blogs/week2/#card-game-design","text":"As discussed in the last week\u2019s post, our strategy is to start with a simple card game and progressively introduce more complexity. This week, we came up with the first iteration of our game and here are the rules. Each player will have the same health amount and attack damage. Only two kinds of cards inside the deck: Heal and Damage. First round each player will have 5 cards and will draw 2 cards each round from the second round. Players will take turns, and they will use all of their cards in hand.","title":"Card Game Design"},{"location":"blogs/week2/#game-kernel","text":"The following is a list of files that we have implemented and what each of them does: GameSimulator.py: Allows us to simulate the game multiple times and view results. GameplayKernel.py: Kernel class that instantiates a game state object to \u2018play\u2019 the card game. commonDef.py: Defines game state related classes in order to have a concise interface for Unity and AI agent training. AutoPlayerDef.py: A bot that randomly picks one card from its hand to play out in each round. constantDef.py: Defines constant values used in our project.","title":"Game Kernel"},{"location":"blogs/week2/#ai-agent","text":"Since we cannot use a tabular approach for our game (as the state-action space is very large), we need to use a neural network. The input layer to the neural network will pass in state defining variable values. We still working on a comprehensive list of such variables. The output layer will then give us a probability distribution over the action space. Using this, we can sample from an action from it or simply take the action that has the highest probability (we plan to try both). The design is of the Policy Gradient RL method. The algorithm that we will begin with is the Advantage Actor Critic. The reason for beginning with policy based methods (over value based methods) is that they perform better in stochastic environments. This applies to our project, because the same actions can result in a different future states based on the moves made by the opponent. During the course of our project, it is likely that we shall try several different approaches to train our AI. We are open to trying value based methods if the Advantage Actor Critic algorithm does not perform well in our setting. Another important aspect to consider is the number of states that we pass into the input layer during training. Since playing a particular card could lead to victory multiple turns later, the reward received after taking a particular action must include discounted future rewards as well. This means that we follow a N-step reward where N is the number of future steps to consider for the reward.","title":"AI Agent"},{"location":"blogs/week2/#visualization-and-input-handling-in-unity","text":"There are several parts to the Unity system that we are building alongside with our Python game.","title":"Visualization and Input Handling in Unity"},{"location":"blogs/week2/#networking-system","text":"For playtesting reasons, we want to allow people to play this game remotely. To that end, we are building a networked Unity system that can allow people to provide inputs remotely through the client. We are using Photon to do this. The reasons for using Photon: Provides free server usage within a limit. Provides easy-to-use API for building room/lobby-based game service. Great support and tutorial resources in Unity. Using Photon, we have built a basic matching system that works for us. It currently includes functionality for: Connecting players to the service. Manage the connection and disconnection. Logic of \u2018Ready\u2019, \u2018Start Game\u2019, and player chat. Below are some screenshots of how this looks currently.","title":"Networking System"},{"location":"blogs/week2/#rules-of-communication-between-python-and-unity-c","text":"We have discussed the rules of how AI, Unity logic, python gameplayer will communicate (to ensure less work when we connect everything). Definition and format of important classes such as gamestate/user input is predefined and stored in a python file (commonDef.py). C#/Python communication: Only C# accesses Python, not the other way around C# can only do the following: Call functions, share data structures, and get the snapshot of object instances in python. C# and Python will share the definition of classes which are commonly referenced by both. Const values and settings will be stored in separate files(XML, JSON, etc.) and will be dynamically loaded when required.","title":"Rules of communication between Python and Unity (C#)"},{"location":"blogs/week2/#c-coding-standard","text":"We added a coding style guide of Unity/C# in our Github repository\u2019s readme file, including rules for class layout, nomenclature, declaration and brace style.","title":"C# Coding Standard"},{"location":"blogs/week3/","text":"Week 3 : The Card Game The biggest highlight this was the pivot. Instead of designing our own game, we decided to go with the popular card game \u2018Slay the Spire\u2019. There were several reasons for doing this. During a recently held Brainstorming Workshop that involved a lot of second year students, we got some feedback that resembled a lot of the same things we had been hearing from our faculty instructors and others who we had described our project to. Since we were implementing the AI and designing the game at the same time, it would have been easy for us to change the game\u2019s design in order to make it more suitable for AI training. This has never been the purpose of our project. We dont want to be changing the game in order to suit the AI. We want the game and its design to drive the AI instead of the other way round. This is because we want to work towards something that can be extended to other games and other settings. And although we are not looking towards developing an AI that can play multiple card games (this is an extremely difficult task), we need to be able to develop AI for a game not designed by us to convince anyone that our work has merit. The Card Game \u2013 Slay the Spire The game that we chose is Slay the Spire . There are several reasons for choosing this game: Simple and Well Designed : Slay the Spire is a well designed game that is a lot of fun to play. The buff system is uniform and consistent which makes it a good setting for AI training. There arent a lot of programming exceptions when it comes to programming which makes the implementation easier. PvE : The game is not player versus player which makes the gameplay more deterministic. With the game concept we had earlier, we were heading towards adversarial AI agents that compete with each other to win. Instead, here we have the AI agent playing against a scripted boss. This makes the training process less complex because the environment is relatively much less stochastic. Data Driven Design : Each card is basically a data structure with specific values for damage, block and buffs. Thus each card can be stored as its own JSON object and it is simple to modify. It also makes it easier for us to add new cards. That being said, we are not looking to build an AI system for the entire game. We are only looking at boss fights with a predetermined deck of cards. As a result, we are only looking to train the AI to play out boss fights. If the AI can learn how to play the boss fight, we can get an understanding of how easy or difficult it is to beat the boss when the damage, block or buff values are altered. Game Jam \u2013 Implementation After deciding that we wanted a already popular card game and then choosing \u2018Slay the Spire\u2019, it was time to move towards the implementation. We had already worked towards the implementation of the previous game and as a result we didnt like the fact that we were back to square one. To overcome this, we decided to do a game jam (Wednesday to Friday) where all three programmers drop everything else and work together to quickly implement a playable version of Slay the Spire. The following are some of the requirements we came up with for this rapid implementation to direct us: Friendly for AI to play, and also can by played by humans using the terminal Clear protocol for the deck and cards for future extensibility (ties into the Data Driven Design which we wanted to preserve) Flexible and agile. It is a rapidly made prototype, but the code will serve as the foundation for what we do in the future Calling this our own little game jam was a great idea. We quickly divided up the work and started while staying connected on Discord to answer each other\u2019s questions. After three long days, we had a terminal-playable prototype of the card game on our hands. It consisted of one boss combat ( The Guardian ) and twelve cards for the player to choose from. The cards that we implemented are as follows: Anger Defend Shrug It Off Pommel Strike Sword Boomerang Body Slam Iron Wave Heavy Blade Flex Double Tap Disarm Clothesline Currently this game is human playable in the terminal. This will change in the future and the game will be playable through a UI. However, for now the play experience is a little more tedious with the player having to choose every card to play one by one. The following is a screenshot of how that looks. Here is a list of different game systems that have been implemented so far: GameManager for managing the game flow, providing API for AI and input management Deck management, which provides the ability import cards and deck information from JSON files Behavior system for entities such as player and enemy Implementation of a decision tree for the boss, which has different modes and various tactics. The next steps include getting back to developing an AI that can play this game. We are simultaneously getting ready for quarters next which will give us a great chance to get feedback about our plan from ETC faculty.","title":"Week 3 : The Card Game"},{"location":"blogs/week3/#week-3-the-card-game","text":"The biggest highlight this was the pivot. Instead of designing our own game, we decided to go with the popular card game \u2018Slay the Spire\u2019. There were several reasons for doing this. During a recently held Brainstorming Workshop that involved a lot of second year students, we got some feedback that resembled a lot of the same things we had been hearing from our faculty instructors and others who we had described our project to. Since we were implementing the AI and designing the game at the same time, it would have been easy for us to change the game\u2019s design in order to make it more suitable for AI training. This has never been the purpose of our project. We dont want to be changing the game in order to suit the AI. We want the game and its design to drive the AI instead of the other way round. This is because we want to work towards something that can be extended to other games and other settings. And although we are not looking towards developing an AI that can play multiple card games (this is an extremely difficult task), we need to be able to develop AI for a game not designed by us to convince anyone that our work has merit.","title":"Week 3 : The Card Game"},{"location":"blogs/week3/#the-card-game-slay-the-spire","text":"The game that we chose is Slay the Spire . There are several reasons for choosing this game: Simple and Well Designed : Slay the Spire is a well designed game that is a lot of fun to play. The buff system is uniform and consistent which makes it a good setting for AI training. There arent a lot of programming exceptions when it comes to programming which makes the implementation easier. PvE : The game is not player versus player which makes the gameplay more deterministic. With the game concept we had earlier, we were heading towards adversarial AI agents that compete with each other to win. Instead, here we have the AI agent playing against a scripted boss. This makes the training process less complex because the environment is relatively much less stochastic. Data Driven Design : Each card is basically a data structure with specific values for damage, block and buffs. Thus each card can be stored as its own JSON object and it is simple to modify. It also makes it easier for us to add new cards. That being said, we are not looking to build an AI system for the entire game. We are only looking at boss fights with a predetermined deck of cards. As a result, we are only looking to train the AI to play out boss fights. If the AI can learn how to play the boss fight, we can get an understanding of how easy or difficult it is to beat the boss when the damage, block or buff values are altered.","title":"The Card Game \u2013 Slay the Spire"},{"location":"blogs/week3/#game-jam-implementation","text":"After deciding that we wanted a already popular card game and then choosing \u2018Slay the Spire\u2019, it was time to move towards the implementation. We had already worked towards the implementation of the previous game and as a result we didnt like the fact that we were back to square one. To overcome this, we decided to do a game jam (Wednesday to Friday) where all three programmers drop everything else and work together to quickly implement a playable version of Slay the Spire. The following are some of the requirements we came up with for this rapid implementation to direct us: Friendly for AI to play, and also can by played by humans using the terminal Clear protocol for the deck and cards for future extensibility (ties into the Data Driven Design which we wanted to preserve) Flexible and agile. It is a rapidly made prototype, but the code will serve as the foundation for what we do in the future Calling this our own little game jam was a great idea. We quickly divided up the work and started while staying connected on Discord to answer each other\u2019s questions. After three long days, we had a terminal-playable prototype of the card game on our hands. It consisted of one boss combat ( The Guardian ) and twelve cards for the player to choose from. The cards that we implemented are as follows: Anger Defend Shrug It Off Pommel Strike Sword Boomerang Body Slam Iron Wave Heavy Blade Flex Double Tap Disarm Clothesline Currently this game is human playable in the terminal. This will change in the future and the game will be playable through a UI. However, for now the play experience is a little more tedious with the player having to choose every card to play one by one. The following is a screenshot of how that looks. Here is a list of different game systems that have been implemented so far: GameManager for managing the game flow, providing API for AI and input management Deck management, which provides the ability import cards and deck information from JSON files Behavior system for entities such as player and enemy Implementation of a decision tree for the boss, which has different modes and various tactics. The next steps include getting back to developing an AI that can play this game. We are simultaneously getting ready for quarters next which will give us a great chance to get feedback about our plan from ETC faculty.","title":"Game Jam \u2013 Implementation"},{"location":"blogs/week4/","text":"Week 4 : Dawn Of The AI This week involved many things. We had a chance to interact with many ETC faculty during the 1/4 walkarounds. This was a great chance for us to showcase what we had accomplished to far and talk about our direction. We ran AI training multiple times with different model structures and hyperparameter values. Unfortunately, we could never achieve a win rate of more than 15.7% which means that we still have a lot of work to do towards the machine learning aspect of the project. Meanwhile, we also came up with a Unity frontend system for visualizing the gameplay and created a Django (python) website on localserver that gives designers a simple GUI to edit card values and even create new ones. Feedback from Quarter Walkarounds Here\u2019s a list of some of the most important feedback we received: We need to come up with one or two metrics that we \u2018target\u2019. By targeting a metric, we want to measure the impact of making game balance updates on these metrics. For example, one of the metrics that we are leaning towards is win rate. By focusing on win rate, we can put game balance changes in perspective to judge whats good and whats not. We need to reach out people in the reinforcement learning space who can help us out with the AI part. Since this space is technically challenging and it is is still quite unexplored, it would be a good idea to get in touch with someone more experienced in the space. We need to create an organized system that can help game designers interact with the AI. This can be to help with inference from general statistics by creating data visualizations or even an interface to train the AI after making game design updates. We need to write a paper as our final deliverable. This makes sense because what we are doing is highly exploratory. A good to extract value from the work that we have done is to document it all for people who may want to work in the same space after us. AI Training Experiments This week involved a lot of AI training experiments. In all honesty, none of our experiments were successful. The highest win rate that we ever got was 15.7% and that is not very good. However, it is still a little better than a random bot. There were two models that we implemented this week: Model 1 : Single Model With All Input Data This is a single model that takes in all of the state input at the same time. This results in 98 input neurons that look like the following: The output consists of estimated Q-values for each card. The neural network is setup to estimate the expected reward values from playing each card given a particular state. The AI agent will then go ahead and play the playable card with the highest Q-value. This does not always mean that the card with highest Q-value gets played since it might not be possible to play that card (player energy, card not in hand, etc). There were several issues with this model that we had to iron out as we ran training. For one, the estimated Q-values were getting very large and would eventually resulted in a Nan error. This was rectified by changing the activation function of the middle layers to sigmoid instead of relu. We also tried regularization as a way to handle this but that resulted in the estimated Q-values being too small. All in all, we got the model to work but it did not show any improvement in the average reward over the duration of the training. Perhaps, it is difficult for the AI to infer insights from the data because too much of it is being presented at once inconsistently. Model 2 : Multiple Small Models Each Predicting Independently The other model we implemented involved getting rid of a single big model but instead making multiple small models. For the sake of experimentation, we created three smaller models as follows: Buff Model \u2013 Consists of 7 input neurons, each indicating whether a certain buff is present on the player (or boss). Cards Model \u2013 Consists of 13 input neurons indicating which cards are in the players hand. One additional neuron to indicate player\u2019s energy level. Boss Intent Model \u2013 Consists of 7 inputs neurons, indicating what is boss\u2019s intent. Out of the 98 values that indicate the game\u2019s current state in model 1, here we are only taking 7 + 13 + 7 of them. By filtering out some of the information, we are losing out out on the AI\u2019s knowledge of the state space but at the same time we want to try this out to see if the AI does any better. By taking a weighted average of the independent Q-values predicted by these three smaller models, we were able to get a win rate of 15.7% which is higher than the random bot. This is good news, because we know that we are doing in the right direction. However, with that being said, the performance is still quite bad. We are now looking to implementing a more complex and fine-tuned reward function that could potentially help with improvement of training. If that too does not prove to be helpful, we shall look towards policy gradient algorithms to try and solve this problem with a fresh approach. Unity Visualization We made some progress on our quest to create a Unity visualization of the game. In the last week (Week 3) we had implemented a python version of the game \u2018Slay the Spire\u2019 which involved a single boss fight against the level 1 boss \u2018The Guardian\u2019. For starters, we worked towards implementing a UI for playing the game which looks like the below. The following are some key features of the Unity system: Decoupled Front End : Front-end includes all animations, UI, and graphics, but does not have any control over the gameplay logic. Data Driven Rendering : Rendering of the game is based on data, just like browsers render HTML. So when we change game logic in python, the rendering part would change according to the data sent back from python. This ensures that we dont need to modify the python code too much. Art Resources Hot Update : Art-side resources shouldn\u2019t be embedded in the application. All of them are configurable and can be hot updated. Art assets can also be provided as tools (for the game designer) and results can be immediately seen without restarting the application. Playback System and Debugger : Support for a playback system based on log data. This is to see how AI plays the game by visualizing its moves. The implementation of the Unity Visualization system follows a front-end/back-end architecture. This is largely inspired from the modern web stack: HTML/Browser/CSS/JS because we have a similar situation. Instead of a game application, Unity is simply a renderer along with some input management. It renders the information from the python module, and gathers user input to send back to python. It is very similar to the relationship between browser and server. The following is a diagram of how this architecture looks: Web GUI for Card Modifications An important piece of our project is to make our system more friendly for game designers. To that end, we began work towards a web GUI for editing the json card files. The idea behind this system is to let the users modify existing cards or create new cards. This system uses a Django backend that is hosted on localserver for editing the card files locally. The following is a screenshot of how this looks right now: The fields in the image are from the card json files. These are all values that would need to be edited by opening up the json file. Instead, this system can also be used to make similar changes. In conclusion, this week involved progress on multiple fronts. However, the biggest concern right now is that the AI is not performing well. In the coming week, more of our efforts will likely be to push towards success in this area.","title":"Week 4 : Dawn Of The AI"},{"location":"blogs/week4/#week-4-dawn-of-the-ai","text":"This week involved many things. We had a chance to interact with many ETC faculty during the 1/4 walkarounds. This was a great chance for us to showcase what we had accomplished to far and talk about our direction. We ran AI training multiple times with different model structures and hyperparameter values. Unfortunately, we could never achieve a win rate of more than 15.7% which means that we still have a lot of work to do towards the machine learning aspect of the project. Meanwhile, we also came up with a Unity frontend system for visualizing the gameplay and created a Django (python) website on localserver that gives designers a simple GUI to edit card values and even create new ones.","title":"Week 4 : Dawn Of The AI"},{"location":"blogs/week4/#feedback-from-quarter-walkarounds","text":"Here\u2019s a list of some of the most important feedback we received: We need to come up with one or two metrics that we \u2018target\u2019. By targeting a metric, we want to measure the impact of making game balance updates on these metrics. For example, one of the metrics that we are leaning towards is win rate. By focusing on win rate, we can put game balance changes in perspective to judge whats good and whats not. We need to reach out people in the reinforcement learning space who can help us out with the AI part. Since this space is technically challenging and it is is still quite unexplored, it would be a good idea to get in touch with someone more experienced in the space. We need to create an organized system that can help game designers interact with the AI. This can be to help with inference from general statistics by creating data visualizations or even an interface to train the AI after making game design updates. We need to write a paper as our final deliverable. This makes sense because what we are doing is highly exploratory. A good to extract value from the work that we have done is to document it all for people who may want to work in the same space after us.","title":"Feedback from Quarter Walkarounds"},{"location":"blogs/week4/#ai-training-experiments","text":"This week involved a lot of AI training experiments. In all honesty, none of our experiments were successful. The highest win rate that we ever got was 15.7% and that is not very good. However, it is still a little better than a random bot. There were two models that we implemented this week:","title":"AI Training Experiments"},{"location":"blogs/week4/#model-1-single-model-with-all-input-data","text":"This is a single model that takes in all of the state input at the same time. This results in 98 input neurons that look like the following: The output consists of estimated Q-values for each card. The neural network is setup to estimate the expected reward values from playing each card given a particular state. The AI agent will then go ahead and play the playable card with the highest Q-value. This does not always mean that the card with highest Q-value gets played since it might not be possible to play that card (player energy, card not in hand, etc). There were several issues with this model that we had to iron out as we ran training. For one, the estimated Q-values were getting very large and would eventually resulted in a Nan error. This was rectified by changing the activation function of the middle layers to sigmoid instead of relu. We also tried regularization as a way to handle this but that resulted in the estimated Q-values being too small. All in all, we got the model to work but it did not show any improvement in the average reward over the duration of the training. Perhaps, it is difficult for the AI to infer insights from the data because too much of it is being presented at once inconsistently.","title":"Model 1 : Single Model With All Input Data"},{"location":"blogs/week4/#model-2-multiple-small-models-each-predicting-independently","text":"The other model we implemented involved getting rid of a single big model but instead making multiple small models. For the sake of experimentation, we created three smaller models as follows: Buff Model \u2013 Consists of 7 input neurons, each indicating whether a certain buff is present on the player (or boss). Cards Model \u2013 Consists of 13 input neurons indicating which cards are in the players hand. One additional neuron to indicate player\u2019s energy level. Boss Intent Model \u2013 Consists of 7 inputs neurons, indicating what is boss\u2019s intent. Out of the 98 values that indicate the game\u2019s current state in model 1, here we are only taking 7 + 13 + 7 of them. By filtering out some of the information, we are losing out out on the AI\u2019s knowledge of the state space but at the same time we want to try this out to see if the AI does any better. By taking a weighted average of the independent Q-values predicted by these three smaller models, we were able to get a win rate of 15.7% which is higher than the random bot. This is good news, because we know that we are doing in the right direction. However, with that being said, the performance is still quite bad. We are now looking to implementing a more complex and fine-tuned reward function that could potentially help with improvement of training. If that too does not prove to be helpful, we shall look towards policy gradient algorithms to try and solve this problem with a fresh approach.","title":"Model 2 : Multiple Small Models Each Predicting Independently"},{"location":"blogs/week4/#unity-visualization","text":"We made some progress on our quest to create a Unity visualization of the game. In the last week (Week 3) we had implemented a python version of the game \u2018Slay the Spire\u2019 which involved a single boss fight against the level 1 boss \u2018The Guardian\u2019. For starters, we worked towards implementing a UI for playing the game which looks like the below. The following are some key features of the Unity system: Decoupled Front End : Front-end includes all animations, UI, and graphics, but does not have any control over the gameplay logic. Data Driven Rendering : Rendering of the game is based on data, just like browsers render HTML. So when we change game logic in python, the rendering part would change according to the data sent back from python. This ensures that we dont need to modify the python code too much. Art Resources Hot Update : Art-side resources shouldn\u2019t be embedded in the application. All of them are configurable and can be hot updated. Art assets can also be provided as tools (for the game designer) and results can be immediately seen without restarting the application. Playback System and Debugger : Support for a playback system based on log data. This is to see how AI plays the game by visualizing its moves. The implementation of the Unity Visualization system follows a front-end/back-end architecture. This is largely inspired from the modern web stack: HTML/Browser/CSS/JS because we have a similar situation. Instead of a game application, Unity is simply a renderer along with some input management. It renders the information from the python module, and gathers user input to send back to python. It is very similar to the relationship between browser and server. The following is a diagram of how this architecture looks:","title":"Unity Visualization"},{"location":"blogs/week4/#web-gui-for-card-modifications","text":"An important piece of our project is to make our system more friendly for game designers. To that end, we began work towards a web GUI for editing the json card files. The idea behind this system is to let the users modify existing cards or create new cards. This system uses a Django backend that is hosted on localserver for editing the card files locally. The following is a screenshot of how this looks right now: The fields in the image are from the card json files. These are all values that would need to be edited by opening up the json file. Instead, this system can also be used to make similar changes. In conclusion, this week involved progress on multiple fronts. However, the biggest concern right now is that the AI is not performing well. In the coming week, more of our efforts will likely be to push towards success in this area.","title":"Web GUI for Card Modifications"},{"location":"blogs/week5/","text":"Week 5 : More Experiments As the title suggests, this week involved a lot of experiments. The bad news is that the AI is still not close to performing optimally. We have not been able to achieve convergence in any of our experimental setups yet. However, on the positive side, we are getting closer to having a graphical interface for playing the game (one boss fight of Slay the Spire). Once this is up, we can send this out to others to test and it can help us visualize how the AI is playing our game. We are adding a replay and recording functionality into the graphical interface to achieve this. Additionally, we identified a few bugs in our game and dedicated some time to fix them as well. Experiments with the AI Reward Calculation Modifications The week started with us making an important change to the AI which involved updating the reward function to manually calculate the rewards of certain cards that involve buffs (Flex, Double Tap, Disarm, Clothesline) and a couple of block cards (Defend, Shrug It Off, Iron Wave). There is a base positive reward for every point of damage done. Since these cards do not deal damage directly, it makes sense to calculate through backtracking their effectiveness. Flex \u2013 Playing the flex card gains reward equal to 50% of damage done by attacks played after it in the same turn. This provides an incentive to play it as early as possible. \u201850%\u2019 sure seems high but keep in mind that Flex is a zero cost card hence the value obtained from this card is quite high (thinking about value in terms of damage/energy use). Double Tap \u2013 Gains reward equal to 50% of the damage dealt by the next attack played. Disarm \u2013 Gains reward proportional to the instances of boss damage (throughout the remainder of the game) after this card was played. Clothesline \u2013 Gains reward proportional to the amount of boss damage inflicted on the player in the next two turns after this card has been played. Defend / Shrug It Off / Iron Wave \u2013 Gains reward proportional to the amount of block used (calculated by looking at the next boss turn after playing this card). Block used is the actual block value that mitigated damage from the boss. These reward modifications did not yield any favorable results at first glance which led us to drop them completely in successful experiments. However, we do think that the code used to do these reward modifications can be useful sometime in the future since it is a good way for us to calculate a card\u2019s effectiveness. Choosing Action Functionality The next big thing that we tried this week involved modifying the way the actions were being chosen out of the array of q-values generated by the AI prediction process. Earlier , the way of choosing the best action was to look at the predicted q-values for each card and then choose the card with the highest q-value AND was playable. One of the reasons this could be a potential problem is that the neural network is unaware of this mechanism for the choice of cards and hence it makes predictions believing that all cards are available for it to play. Logically there is an obvious flaw here. However, this effect is even more pronounced when considering the Q-Learning rule for estimating reward ( q(s,a) = reward + max a\u2019 q(s\u2019,a\u2019) ). In words, the equation states that the expected reward from taking action a (playing a card) in state s (the current game state) is equal to the immediate reward from playing the card added to the maximum q-value possible for the next state. Hence, when the AI tries to predict q-values, it assumes that even in the next state, all cards are available to play. This has an inflating effect on the q-values and we have seen q-values accelerating to very high numbers. To fix this, we now force the neural network to choose an action that is playable in the given state. We do this by giving a negative reward when the neural network attributes the highest q-value to a card that is unplayable in the given state. By doing this, the neural network is forced to pick a card that is playable. This has its own set of challenges but for now we are experimenting more in this direction. Curse of Dimensionality (High Number Of States) Although we are breaking down the game state into a few variables (around 100) with discrete values, the complexity of this task has a lot to do with the high number of possible states. Right now we know that there two things for sure: AI Agent is unable to find the best strategy to play the game AI Agent is unable to optimize the card play order in each turn One of the reasons for this could be that it will take a high number of games for the AI to visit enough different states to converge. Right now, it takes about an hour to play 1000 games (each game has multiple turns and each turn has multiple card play steps). One of the things that we are looking at right now is feature engineering. Is there a clever way to extract important information from the game state and represent it in a fewer number of states? The following is what we are looking at: Generalize card states by changing different cards to only genre of cards (Card\u2019s name \u2192 Genre of the cards): Instead of checking for cards on the player\u2019s hand, we only check how many attack cards, how many block cards and how many buff cards are on player\u2019s hand. Player/Boss Buffs: Maybe this does not impact the game state too much and we can remove this altogether. Unity Front End Last week we built a unity front-end system which can only render a static scene. Since animation is so important for players to know what is happening, we added animations to our front end this week! Challenges In a normal unity game, adding animation is easy since we can easily get references to all gameobject instances we want for animations. But since our game is running in python, all the object instances are stored in python runtime. An object instance sharing mechanism is hard to fit into our current request/response architecture between C#/C++. Solution We extended our \u201cmarkup-language\u201d to support animation. All the markups in one \u201cgame sequence markup file\u201d share the same ID space, so they can find instance by those ID. The flow of this is shown below. Playback System in Unity As mentioned in the introduction above, we need a playback system in Unity in order to watch replays of games played by the AI (or humans). This tool can be incredible useful to us because: Store valuable information permanently : Previously we didn\u2019t have efficient ways to store how AI/players play the game but with this system, we can store records as along as we version them and have backward compatibility. Clear graphical visualization : Even a single minute of AI\u2019s gameplay generates hundreds of lines of logs which are hard to read. With graphical interface, life would be easier in evaluating what the AI is doing. Playback Tool based on Current Architecture This week we implemented a playback tool for the reasons cited above that is completely compatible with our current architecture. This includes: Font-Backed Rendering System which completely relies on the game data in markup files. Mechanism to Encode Gameplay Data into markup files. We store the data generated during runtime, and use the same Unity based rendering system to replay it. Refactoring the Data Management Module As part of integrating the python gameplay system with Unity rendering and the replay mechanism, we also implemented a database module to handle game data drawn from markup files. The following are some reasons why we needed to do this: Easier for developers to configure and modify data. This is in preparation of the design tools that we are looking to build. Decoupling data from gameplay is one of the most important aspects of our original design Preparation for game application builds The following design considers support for multiple games versions such that each game version\u2019s data is isolated from others. Each game supports multiple decks, and its own rule sets and card set. The following is a brief description of what the database module handbook looks like: Bug Fixes We identified a few bugs this week during manual playtests and AI training. In the previous version, the boss transformed from offensive mode to defensive mode only when it is attacked by more than 30 damages in one round. In the real Slay the Spire, the damage dealt to the enemy for transformation is accumulated since the last transformation. In order to fix this, we had to take another look at our game flow and make some changes to it. The following is a representation of how our (cleaned up) current game loop looks like: Although the AI training is still not yielding favorable results, we are learning a lot about the application of reinforcement learning to strategy games. It will surely be fun to watch the AI play in the coming week. We can then judge to see how smart it is!","title":"Week 5 : More Experiments"},{"location":"blogs/week5/#week-5-more-experiments","text":"As the title suggests, this week involved a lot of experiments. The bad news is that the AI is still not close to performing optimally. We have not been able to achieve convergence in any of our experimental setups yet. However, on the positive side, we are getting closer to having a graphical interface for playing the game (one boss fight of Slay the Spire). Once this is up, we can send this out to others to test and it can help us visualize how the AI is playing our game. We are adding a replay and recording functionality into the graphical interface to achieve this. Additionally, we identified a few bugs in our game and dedicated some time to fix them as well.","title":"Week 5 : More Experiments"},{"location":"blogs/week5/#experiments-with-the-ai","text":"","title":"Experiments with the AI"},{"location":"blogs/week5/#reward-calculation-modifications","text":"The week started with us making an important change to the AI which involved updating the reward function to manually calculate the rewards of certain cards that involve buffs (Flex, Double Tap, Disarm, Clothesline) and a couple of block cards (Defend, Shrug It Off, Iron Wave). There is a base positive reward for every point of damage done. Since these cards do not deal damage directly, it makes sense to calculate through backtracking their effectiveness. Flex \u2013 Playing the flex card gains reward equal to 50% of damage done by attacks played after it in the same turn. This provides an incentive to play it as early as possible. \u201850%\u2019 sure seems high but keep in mind that Flex is a zero cost card hence the value obtained from this card is quite high (thinking about value in terms of damage/energy use). Double Tap \u2013 Gains reward equal to 50% of the damage dealt by the next attack played. Disarm \u2013 Gains reward proportional to the instances of boss damage (throughout the remainder of the game) after this card was played. Clothesline \u2013 Gains reward proportional to the amount of boss damage inflicted on the player in the next two turns after this card has been played. Defend / Shrug It Off / Iron Wave \u2013 Gains reward proportional to the amount of block used (calculated by looking at the next boss turn after playing this card). Block used is the actual block value that mitigated damage from the boss. These reward modifications did not yield any favorable results at first glance which led us to drop them completely in successful experiments. However, we do think that the code used to do these reward modifications can be useful sometime in the future since it is a good way for us to calculate a card\u2019s effectiveness.","title":"Reward Calculation Modifications"},{"location":"blogs/week5/#choosing-action-functionality","text":"The next big thing that we tried this week involved modifying the way the actions were being chosen out of the array of q-values generated by the AI prediction process. Earlier , the way of choosing the best action was to look at the predicted q-values for each card and then choose the card with the highest q-value AND was playable. One of the reasons this could be a potential problem is that the neural network is unaware of this mechanism for the choice of cards and hence it makes predictions believing that all cards are available for it to play. Logically there is an obvious flaw here. However, this effect is even more pronounced when considering the Q-Learning rule for estimating reward ( q(s,a) = reward + max a\u2019 q(s\u2019,a\u2019) ). In words, the equation states that the expected reward from taking action a (playing a card) in state s (the current game state) is equal to the immediate reward from playing the card added to the maximum q-value possible for the next state. Hence, when the AI tries to predict q-values, it assumes that even in the next state, all cards are available to play. This has an inflating effect on the q-values and we have seen q-values accelerating to very high numbers. To fix this, we now force the neural network to choose an action that is playable in the given state. We do this by giving a negative reward when the neural network attributes the highest q-value to a card that is unplayable in the given state. By doing this, the neural network is forced to pick a card that is playable. This has its own set of challenges but for now we are experimenting more in this direction.","title":"Choosing Action Functionality"},{"location":"blogs/week5/#curse-of-dimensionality-high-number-of-states","text":"Although we are breaking down the game state into a few variables (around 100) with discrete values, the complexity of this task has a lot to do with the high number of possible states. Right now we know that there two things for sure: AI Agent is unable to find the best strategy to play the game AI Agent is unable to optimize the card play order in each turn One of the reasons for this could be that it will take a high number of games for the AI to visit enough different states to converge. Right now, it takes about an hour to play 1000 games (each game has multiple turns and each turn has multiple card play steps). One of the things that we are looking at right now is feature engineering. Is there a clever way to extract important information from the game state and represent it in a fewer number of states? The following is what we are looking at: Generalize card states by changing different cards to only genre of cards (Card\u2019s name \u2192 Genre of the cards): Instead of checking for cards on the player\u2019s hand, we only check how many attack cards, how many block cards and how many buff cards are on player\u2019s hand. Player/Boss Buffs: Maybe this does not impact the game state too much and we can remove this altogether.","title":"Curse of Dimensionality (High Number Of States)"},{"location":"blogs/week5/#unity-front-end","text":"Last week we built a unity front-end system which can only render a static scene. Since animation is so important for players to know what is happening, we added animations to our front end this week!","title":"Unity Front End"},{"location":"blogs/week5/#challenges","text":"In a normal unity game, adding animation is easy since we can easily get references to all gameobject instances we want for animations. But since our game is running in python, all the object instances are stored in python runtime. An object instance sharing mechanism is hard to fit into our current request/response architecture between C#/C++.","title":"Challenges"},{"location":"blogs/week5/#solution","text":"We extended our \u201cmarkup-language\u201d to support animation. All the markups in one \u201cgame sequence markup file\u201d share the same ID space, so they can find instance by those ID. The flow of this is shown below.","title":"Solution"},{"location":"blogs/week5/#playback-system-in-unity","text":"As mentioned in the introduction above, we need a playback system in Unity in order to watch replays of games played by the AI (or humans). This tool can be incredible useful to us because: Store valuable information permanently : Previously we didn\u2019t have efficient ways to store how AI/players play the game but with this system, we can store records as along as we version them and have backward compatibility. Clear graphical visualization : Even a single minute of AI\u2019s gameplay generates hundreds of lines of logs which are hard to read. With graphical interface, life would be easier in evaluating what the AI is doing.","title":"Playback System in Unity"},{"location":"blogs/week5/#playback-tool-based-on-current-architecture","text":"This week we implemented a playback tool for the reasons cited above that is completely compatible with our current architecture. This includes: Font-Backed Rendering System which completely relies on the game data in markup files. Mechanism to Encode Gameplay Data into markup files. We store the data generated during runtime, and use the same Unity based rendering system to replay it.","title":"Playback Tool based on Current Architecture"},{"location":"blogs/week5/#refactoring-the-data-management-module","text":"As part of integrating the python gameplay system with Unity rendering and the replay mechanism, we also implemented a database module to handle game data drawn from markup files. The following are some reasons why we needed to do this: Easier for developers to configure and modify data. This is in preparation of the design tools that we are looking to build. Decoupling data from gameplay is one of the most important aspects of our original design Preparation for game application builds The following design considers support for multiple games versions such that each game version\u2019s data is isolated from others. Each game supports multiple decks, and its own rule sets and card set. The following is a brief description of what the database module handbook looks like:","title":"Refactoring the Data Management Module"},{"location":"blogs/week5/#bug-fixes","text":"We identified a few bugs this week during manual playtests and AI training. In the previous version, the boss transformed from offensive mode to defensive mode only when it is attacked by more than 30 damages in one round. In the real Slay the Spire, the damage dealt to the enemy for transformation is accumulated since the last transformation. In order to fix this, we had to take another look at our game flow and make some changes to it. The following is a representation of how our (cleaned up) current game loop looks like: Although the AI training is still not yielding favorable results, we are learning a lot about the application of reinforcement learning to strategy games. It will surely be fun to watch the AI play in the coming week. We can then judge to see how smart it is!","title":"Bug Fixes"},{"location":"blogs/week6/","text":"Week 6 : A Small Breakthrough This week saw more work along similar lines as the last week. One major improvement for the AI came as a result of making it simpler for our designer to run training. A major algorithm update was also done which helped us improve training results. A whole set of new cards was added to the game for which we had to add some additional reward functions. Training data is now recorded better with a lot of important statistical information and charts being saved in an excel file. On the Unity front, we finally finished the GUI prototype of the boss fight based in Unity. We sent it out to our faculty instructors Mike Christel and Scott Stevens to test. Finally, we also began preparation for halves which is due on 19th October. AI Updates The updates made to the AI can be broken down into the following three parts: Flexible State and Action Spaces Flexible state and action spaces implies that the AI Agent script no longer needs explicit instructions about the state and action space. Instead it picks up this information from JSON files just like other parts of our app. State Space : The state space is constructed by looking at the state space selector file. This is a JSON file that has a bool value attributed with various portions of the state space (player buffs, boss intent, in hand cards, etc.). Before starting training, you can now directly modify this file to choose what should and should not be included in the state space for AI training. The below is what this looks like right now: Action Space : Constructing the action space involved looking into the database module and getting information about the current deck configuration. The action space is then constructed by adding a neuron for each card which has a non-zero count in the deck. There is no input required to directly modify the action space. The reason for adding this feature is to make it much simpler for a designer to run training. Now the designer simply needs to choose cards to add to their deck and update the state space for including states that they want in the state space. This format is also great to integrate with a GUI system that can make doing the above as simple as filling out a form containing different elements of the game state. We needed a system like this to make the AI training more accessible to our designer. There are still many experiments that need to be run and many of them involve modifying the deck and updating the state/action space. AI Algorithm Updates Another important task this week was the modifications made to the A-Learning algorithm. Last week, the major change we made which resulted in stable learning was updating the discounting factor (gamma) to zero when calculating a state\u2019s expected reward. This essentially meant that expected reward of a state is equal to only the immediate reward received from the current state. The reason learning becomes stable by implementing this is that the q-target becomes fixed. Earlier (with gamma > 0), the q-target value for the same state could change as a result of updates made to the q-learning model. But if the discounting factor is made zero, the q-target becomes exactly equal to the immediate reward (the reward is stochastic in our environment as a result of the game\u2019s randomness). This was what we did last week. This week, we wanted to increase gamma to be greater than zero while still ensuring that training remains stable. We did this by adding another model to the AI Agent script. Thus essentially now we have two models. The setting is such that one model is used only for prediction and the other model is used only for training. When the model being trained has gone through enough iterations, the model making predictions is deleted an the trained model takes it place. A fresh model is instantiated to be the new training model. A shorter version of the same is outlined below. Instantiate two models Q-Train and Q-Predict For number of games: Q-Train is initially trained while Q-Predict is used to make predictions (the first Q-Predict model will only make random predictions) If Q-Train has gone through enough iterations: Q-Predict = Q-Train Q-Train = new model Essentially, the purpose this serves is that it keeps the q-targets stationary for a length of time while the Q-Train model is being trained. This stabilizes the training of the Q-Train model. Once the Q-Train model is \u2018experienced enough\u2019, it can start making predictions. Another thing this enables us to do is to make gamma > 0. But this is not done immediately. Instead, the starting gamma is 0 but every time the Q-model is switched, the gamma is made a little higher. Hence, the discounting factor starts off with 0 and increases by 0.2 every time the Q-model makes a switch. This way, every fresh Q-Train model, attributes a little higher weight to the future reward as well as the expected reward. Implementing the above algorithm update has increase the win rate from 0.4% to 7.87%. Although the win rate is still far from good, the AI is definitely performing better and has started winning in a setting that is challenging even for a human (player hp : 75, boss hp : 240). This is the biggest breakthrough at AI training that we have had in a while. As a result, it gives us hope that we are on the right path for making progress towards the goal. Reward Function Updates The last piece of AI updates involves updates made to the reward function. There were bugs in the reward function that resulted in unfairly high rewards to the damage dealing cards. This has been modified and rewards to the buff cards and block cards has been buffed a little. Using a complex reward function like this is something that we want to move away from in the long run. The idea behind reinforcement learning is to train the AI as much as possible with only rewards for achieving goals. What we have ended up doing is giving a lot of intermediary goals to push the AI towards achieving the desired goal. The reason is simply that we have not had much success with rewards only for the final goal and hence we introduced intermediary rewards. However, this is something we would like to change. AI Training Statistics Another significant improvement made in the previous week is the addition of comprehensive AI training statistics as a part of the AI generated data. To do this we had to add many new scripts that deal solely with collecting, processing and writing AI training data. The generated statistics file is now a .xlsx file which includes two worksheets. Training Data : This worksheet provides episode details of each game played by the AI. This includes the following information for each episode: Episode Number Epsilon value for that episode Total cards played in the game (each episode is one game) Total reward for this episode Win/Loss HP of the Boss at the end of the game HP of the Player at the end of the game Max Damage done in any turn of the game Average damage done in the turns of the game Episode indexes when model was switched A graph showing a rolling average of total episode reward (average over 20 games including the current and following 19) Card Statistics : This worksheet provides insights into the performance of each card in all games played during training. This includes the following information for each card in the deck: Action number (this is the number of the action neuron representing the action of playing this card) Card name Count of this card in the deck Card play count : times this card was played in all games of training Opportunities of playing a card utilized. It is a ratio of how many times a card was played divided the number of times it was available Average card play position. At what step in the turn was this card played on average. For example, if a card was always played between the first in all turns, its average card play position would be 1. Average reward obtained by playing this card. Graphs of Card Play Count, Card Play Opportunities Utilized and Card Average Reward Below are screenshots of these excel sheets and what they look like right now: Training Data Worksheet Card Statistics Worksheet These statistics have already given us some great insights into what is happening during training and how the AI is thinking while playing cards. Although this is not an exhaustive list of the statistics we wish to provide the game designer, it is a start towards achieving the greater of training the AI, i.e helping the designer. As the AI training improves, these statistics may prove to be quite valuable to the game designer in making game design decisions. Python Integration Into Unity Build As a result of the game backend being in python and rendering of graphics in Unity, in the past we had two separate applications running at the same time. We figured out how Unity and Python communicate, however, this only works during development. Once we build the project and deliver it to players, it becomes more complicated because unity manages its own resources and builds most of them into binaries or machine code. To figure out how to do this, we analyzed some of the various options available to us. Option 1 \u2013 Remote Server: Python code will not run in the build, instead it will run on a remote server. This is doable to some extent, because we use sockets to communicate between Unity and Python. And our architecture is very similar to a frontend-backend model. Pros: Needs very little work, just run python on another computer as a server when playtesting. More convenient to collect data from playtesters in the background. Cons: The build is not self-contained. Always needs remote server to work. Option 2 \u2013 Integrate Python Executable into Build Use tools like pyinstaller to pack python code into a .exe, and then start that .exe when Unity application starts. Pros: The build is 100% standalone. Cons: Build fails in our project now, using pyinstaller. We need time to make sure the build process works. We wrote our database module in Python. We don\u2019t want to put data (images, configurations) into the python executable directory. Option 3 \u2013 Interpret raw python code files within build We make a unity build first, and then include python code directories into Unity build. During runtime, the Unity application finds the Python interpreter to execute the code. Pros: The process is 100% transparent. We have full control of the directory, resources and data in the build. Cons: Not 100% standalone for now. There is a dependency on the python environment in the user\u2019s computer. Final Decision : (3) Include raw python code files within Build We don\u2019t want to maintain a server which eliminates option 1. Option 2 would be ideal if it works, however, pyinstaller is a big unknown to us and we don\u2019t know how much time we need to fix it We made option 3 work, and in the future we plan to install the required environment for users before starting the application. Playable GUI Version of Game In week 4, we designed and implemented the architecture of combining gameplay in python and GUI in Unity. In week 5, we developed the animation workflow from python to unity. Finally during week 6, having the existing foundation, we made our first playable version! Below is screenshot of how it looks: A lot of work went into making this run. Here are some key highlights of the steps we needed to take: Enriching the C#/Python protocol Previously, our protocol only supported requests of user input and response of markup files for unity to render. Now our protocol supports multiple types like system error, game life cycle update, etc. Additionally, it is also extensible for new types. Resources (card image/tooltip text) management from python database to unity Now we can configure the images, descriptions of cards in the database (python). Unity will read this data according to the configuration! Information display Card information, deck information, description and images of every card, states and information of player and enemies, are all showed to the player. Comprehensive and robust Main gameplay process From connecting to backend, start game, player turn, enemy turn, loss/win and replay. We now have a complete process for playing, recording and replaying the game Input validation The previous version allowed the player to freely click all buttons and send requests to the backend, but these inputs were not validated in the backend. For example, validating if the card played is on your hand or if you have sufficient energy to play it. We recently added input validation to make the application robust. Addition of New Cards In the last week, we added several new cards to the deck. These have been integrated such that they work in the game as well as for training AI. The new cards include a lot of the \u2018plus\u2019 version of our existing cards as well as some newer cards. A list of the new cards is below: Plus versions of existing cards: Anger Plus Body Slam Plus Clothesline Plus Defend Plus Disarm Plus Flex Plus Heavy Blade Plus Pommel Strike Plus Sword Boomerang Plus Shrug It Off Plus Iron Wave Plus The new cards that have been added are: Bash / Bash Plus Bludgeon / Bludgeon Plus Thunderclap / Thunderclap Plus Twin Strike / Twin Strike Plus Uppercut / Uppercut Plus Adding these new cards inches our implementation closer to the real version of Slay the Spire (it is still quite different). It also gives our designer the ability of modify the deck with many different cards and running training with various different deck versions. All in all, we made a lot of significant progress this week. We are excited to show our work at halves and for the future of our project!","title":"Week 6 : A Small Breakthrough"},{"location":"blogs/week6/#week-6-a-small-breakthrough","text":"This week saw more work along similar lines as the last week. One major improvement for the AI came as a result of making it simpler for our designer to run training. A major algorithm update was also done which helped us improve training results. A whole set of new cards was added to the game for which we had to add some additional reward functions. Training data is now recorded better with a lot of important statistical information and charts being saved in an excel file. On the Unity front, we finally finished the GUI prototype of the boss fight based in Unity. We sent it out to our faculty instructors Mike Christel and Scott Stevens to test. Finally, we also began preparation for halves which is due on 19th October.","title":"Week 6 : A Small Breakthrough"},{"location":"blogs/week6/#ai-updates","text":"The updates made to the AI can be broken down into the following three parts: Flexible State and Action Spaces Flexible state and action spaces implies that the AI Agent script no longer needs explicit instructions about the state and action space. Instead it picks up this information from JSON files just like other parts of our app. State Space : The state space is constructed by looking at the state space selector file. This is a JSON file that has a bool value attributed with various portions of the state space (player buffs, boss intent, in hand cards, etc.). Before starting training, you can now directly modify this file to choose what should and should not be included in the state space for AI training. The below is what this looks like right now: Action Space : Constructing the action space involved looking into the database module and getting information about the current deck configuration. The action space is then constructed by adding a neuron for each card which has a non-zero count in the deck. There is no input required to directly modify the action space. The reason for adding this feature is to make it much simpler for a designer to run training. Now the designer simply needs to choose cards to add to their deck and update the state space for including states that they want in the state space. This format is also great to integrate with a GUI system that can make doing the above as simple as filling out a form containing different elements of the game state. We needed a system like this to make the AI training more accessible to our designer. There are still many experiments that need to be run and many of them involve modifying the deck and updating the state/action space.","title":"AI Updates"},{"location":"blogs/week6/#ai-algorithm-updates","text":"Another important task this week was the modifications made to the A-Learning algorithm. Last week, the major change we made which resulted in stable learning was updating the discounting factor (gamma) to zero when calculating a state\u2019s expected reward. This essentially meant that expected reward of a state is equal to only the immediate reward received from the current state. The reason learning becomes stable by implementing this is that the q-target becomes fixed. Earlier (with gamma > 0), the q-target value for the same state could change as a result of updates made to the q-learning model. But if the discounting factor is made zero, the q-target becomes exactly equal to the immediate reward (the reward is stochastic in our environment as a result of the game\u2019s randomness). This was what we did last week. This week, we wanted to increase gamma to be greater than zero while still ensuring that training remains stable. We did this by adding another model to the AI Agent script. Thus essentially now we have two models. The setting is such that one model is used only for prediction and the other model is used only for training. When the model being trained has gone through enough iterations, the model making predictions is deleted an the trained model takes it place. A fresh model is instantiated to be the new training model. A shorter version of the same is outlined below. Instantiate two models Q-Train and Q-Predict For number of games: Q-Train is initially trained while Q-Predict is used to make predictions (the first Q-Predict model will only make random predictions) If Q-Train has gone through enough iterations: Q-Predict = Q-Train Q-Train = new model Essentially, the purpose this serves is that it keeps the q-targets stationary for a length of time while the Q-Train model is being trained. This stabilizes the training of the Q-Train model. Once the Q-Train model is \u2018experienced enough\u2019, it can start making predictions. Another thing this enables us to do is to make gamma > 0. But this is not done immediately. Instead, the starting gamma is 0 but every time the Q-model is switched, the gamma is made a little higher. Hence, the discounting factor starts off with 0 and increases by 0.2 every time the Q-model makes a switch. This way, every fresh Q-Train model, attributes a little higher weight to the future reward as well as the expected reward. Implementing the above algorithm update has increase the win rate from 0.4% to 7.87%. Although the win rate is still far from good, the AI is definitely performing better and has started winning in a setting that is challenging even for a human (player hp : 75, boss hp : 240). This is the biggest breakthrough at AI training that we have had in a while. As a result, it gives us hope that we are on the right path for making progress towards the goal.","title":"AI Algorithm Updates"},{"location":"blogs/week6/#reward-function-updates","text":"The last piece of AI updates involves updates made to the reward function. There were bugs in the reward function that resulted in unfairly high rewards to the damage dealing cards. This has been modified and rewards to the buff cards and block cards has been buffed a little. Using a complex reward function like this is something that we want to move away from in the long run. The idea behind reinforcement learning is to train the AI as much as possible with only rewards for achieving goals. What we have ended up doing is giving a lot of intermediary goals to push the AI towards achieving the desired goal. The reason is simply that we have not had much success with rewards only for the final goal and hence we introduced intermediary rewards. However, this is something we would like to change.","title":"Reward Function Updates"},{"location":"blogs/week6/#ai-training-statistics","text":"Another significant improvement made in the previous week is the addition of comprehensive AI training statistics as a part of the AI generated data. To do this we had to add many new scripts that deal solely with collecting, processing and writing AI training data. The generated statistics file is now a .xlsx file which includes two worksheets. Training Data : This worksheet provides episode details of each game played by the AI. This includes the following information for each episode: Episode Number Epsilon value for that episode Total cards played in the game (each episode is one game) Total reward for this episode Win/Loss HP of the Boss at the end of the game HP of the Player at the end of the game Max Damage done in any turn of the game Average damage done in the turns of the game Episode indexes when model was switched A graph showing a rolling average of total episode reward (average over 20 games including the current and following 19) Card Statistics : This worksheet provides insights into the performance of each card in all games played during training. This includes the following information for each card in the deck: Action number (this is the number of the action neuron representing the action of playing this card) Card name Count of this card in the deck Card play count : times this card was played in all games of training Opportunities of playing a card utilized. It is a ratio of how many times a card was played divided the number of times it was available Average card play position. At what step in the turn was this card played on average. For example, if a card was always played between the first in all turns, its average card play position would be 1. Average reward obtained by playing this card. Graphs of Card Play Count, Card Play Opportunities Utilized and Card Average Reward Below are screenshots of these excel sheets and what they look like right now: Training Data Worksheet Card Statistics Worksheet These statistics have already given us some great insights into what is happening during training and how the AI is thinking while playing cards. Although this is not an exhaustive list of the statistics we wish to provide the game designer, it is a start towards achieving the greater of training the AI, i.e helping the designer. As the AI training improves, these statistics may prove to be quite valuable to the game designer in making game design decisions.","title":"AI Training Statistics"},{"location":"blogs/week6/#python-integration-into-unity-build","text":"As a result of the game backend being in python and rendering of graphics in Unity, in the past we had two separate applications running at the same time. We figured out how Unity and Python communicate, however, this only works during development. Once we build the project and deliver it to players, it becomes more complicated because unity manages its own resources and builds most of them into binaries or machine code. To figure out how to do this, we analyzed some of the various options available to us.","title":"Python Integration Into Unity Build"},{"location":"blogs/week6/#option-1-remote-server","text":"Python code will not run in the build, instead it will run on a remote server. This is doable to some extent, because we use sockets to communicate between Unity and Python. And our architecture is very similar to a frontend-backend model. Pros: Needs very little work, just run python on another computer as a server when playtesting. More convenient to collect data from playtesters in the background. Cons: The build is not self-contained. Always needs remote server to work.","title":"Option 1 \u2013 Remote Server:"},{"location":"blogs/week6/#option-2-integrate-python-executable-into-build","text":"Use tools like pyinstaller to pack python code into a .exe, and then start that .exe when Unity application starts. Pros: The build is 100% standalone. Cons: Build fails in our project now, using pyinstaller. We need time to make sure the build process works. We wrote our database module in Python. We don\u2019t want to put data (images, configurations) into the python executable directory.","title":"Option 2  \u2013 Integrate Python Executable into Build"},{"location":"blogs/week6/#option-3-interpret-raw-python-code-files-within-build","text":"We make a unity build first, and then include python code directories into Unity build. During runtime, the Unity application finds the Python interpreter to execute the code. Pros: The process is 100% transparent. We have full control of the directory, resources and data in the build. Cons: Not 100% standalone for now. There is a dependency on the python environment in the user\u2019s computer.","title":"Option 3 \u2013 Interpret raw python code files within build"},{"location":"blogs/week6/#final-decision-3-include-raw-python-code-files-within-build","text":"We don\u2019t want to maintain a server which eliminates option 1. Option 2 would be ideal if it works, however, pyinstaller is a big unknown to us and we don\u2019t know how much time we need to fix it We made option 3 work, and in the future we plan to install the required environment for users before starting the application.","title":"Final Decision : (3) Include raw python code files within Build"},{"location":"blogs/week6/#playable-gui-version-of-game","text":"In week 4, we designed and implemented the architecture of combining gameplay in python and GUI in Unity. In week 5, we developed the animation workflow from python to unity. Finally during week 6, having the existing foundation, we made our first playable version! Below is screenshot of how it looks: A lot of work went into making this run. Here are some key highlights of the steps we needed to take:","title":"Playable GUI Version of Game"},{"location":"blogs/week6/#enriching-the-cpython-protocol","text":"Previously, our protocol only supported requests of user input and response of markup files for unity to render. Now our protocol supports multiple types like system error, game life cycle update, etc. Additionally, it is also extensible for new types.","title":"Enriching the C#/Python protocol"},{"location":"blogs/week6/#resources-card-imagetooltip-text-management-from-python-database-to-unity","text":"Now we can configure the images, descriptions of cards in the database (python). Unity will read this data according to the configuration!","title":"Resources (card image/tooltip text) management from python database to unity"},{"location":"blogs/week6/#information-display","text":"Card information, deck information, description and images of every card, states and information of player and enemies, are all showed to the player.","title":"Information display"},{"location":"blogs/week6/#comprehensive-and-robust-main-gameplay-process","text":"From connecting to backend, start game, player turn, enemy turn, loss/win and replay. We now have a complete process for playing, recording and replaying the game","title":"Comprehensive and robust Main gameplay process"},{"location":"blogs/week6/#input-validation","text":"The previous version allowed the player to freely click all buttons and send requests to the backend, but these inputs were not validated in the backend. For example, validating if the card played is on your hand or if you have sufficient energy to play it. We recently added input validation to make the application robust.","title":"Input validation"},{"location":"blogs/week6/#addition-of-new-cards","text":"In the last week, we added several new cards to the deck. These have been integrated such that they work in the game as well as for training AI. The new cards include a lot of the \u2018plus\u2019 version of our existing cards as well as some newer cards. A list of the new cards is below: Plus versions of existing cards: Anger Plus Body Slam Plus Clothesline Plus Defend Plus Disarm Plus Flex Plus Heavy Blade Plus Pommel Strike Plus Sword Boomerang Plus Shrug It Off Plus Iron Wave Plus The new cards that have been added are: Bash / Bash Plus Bludgeon / Bludgeon Plus Thunderclap / Thunderclap Plus Twin Strike / Twin Strike Plus Uppercut / Uppercut Plus Adding these new cards inches our implementation closer to the real version of Slay the Spire (it is still quite different). It also gives our designer the ability of modify the deck with many different cards and running training with various different deck versions. All in all, we made a lot of significant progress this week. We are excited to show our work at halves and for the future of our project!","title":"Addition of New Cards"},{"location":"blogs/week7/","text":"Week 7 : A Little Bit of Success This was the week before halves and all of us spent a considerable amount of effort in preparing for it. However, we also made some considerable progress in both the AI training as well as the Unity visualizer. Most importantly, we managed to improve the AI\u2019s winrate to ~60% which is significantly higher than the previous week\u2019s ~13%. This came as a result of several bug fixes, addition of a new input feature and a modification while calculating q-target values. As part of the Unity visualizer, we can now integrate a trained AI with it to see how the AI predict\u2019s expected rewards from playing cards in a turn. Along with this, we also implemented a state editor tool which allows us to create our own states within Unity that can be very useful in seeing the AI\u2019s reactions in particular states. Lets go over each of these one by one. AI Algorithm Updates The list of things we tried and tested this week are as follows: Addition of \u2018Remaining Damage for Boss Mode Change\u2019 as an Input Feature The Guadian (boss in our implementation) in Slay the Spire has two distinct modes offensive and defensive. It also has specific intents for each phase. When the Guardian is in offensive mode, it generally deals a higher damage than when it is in defensive mode. The switch from offensive to defensive modes happens as a result of damage from the player and generally this is a tool for the player to force the boss into the stance which is favorable to the player. This feature had been left out in our initial models which could have been one of the contributing factors to why the AI was performing poorly. Luckily our faculty instructor Mike Christel pointed this out during one of our faculty meetings where he saw the state space and found this value missing. Addition of this feature to our state space did not immediately result in improved results, however we believe that it was a contributing factor that propelled the winrate higher. Action Masking An approach we focused on this week came as a suggestion from a conversation with a PhD student on main campus James Cunningham. As another strategy to teach the agent about unplayable cards quickly, we decided not only to adjust the q-value of the card that was played but also to adjust the q-values of all the unplayable cards. How do we calculate the target q-values for unplayable cards? Well, since these cards are unplayable we should give them the same negative reward that we give the agent when it tried to play an unplayable card. This means that there would be a negative reward associated with all unplayable cards while constructing the q-target vector only the playable cards could have positive values. Since the largest negative reward is for trying to play unplayable cards and losing, the agent is motivated to quickly learn which cards are unplayable. Earlier, we saw that the agent generally took ~1000-2000 games to learn to not play unplayable cards. With action masking implemented, it now takes ~200-300 to learn to not play unplayable cards. This is a big leap and it gives the agent more time to learn and explore the actual game. Testing different values for Number of Iterations before Q-models switch Last week we introduced the idea of using incremental gamma values along with 2 q-models for our training process. This week we tried an experiment to check what is a good number of iterations for the q-train model to train for before we make the q-model switch. We tested with 10,000 iterations before switching and this resulted in a very poor winrate of ~10%. This is significantly lower than the ~55% winrate that we got with 100,000+ iterations. We also tested with 500,000 iterations and this also gave us a winrate of ~60% implying that increasing the number of iterations too much does not improve the winrate considerably. Ideation towards our Playable Prototype One of our deliverables for the project is to create a playable prototype that is accessible for designers, can run AI trainings and can also provide an interface to load up the Unity visualizer. Our current plan is to have a home page which is similar to a central hub that controls everything else. This would contain paths to the following: Create & edit cards page: enable designers to create and edit cards. Deck building page: configure what and how many cards to include in a version of the deck. Modify AI training parameters page: set hyperparameters like gamma (discounting factor when calculating rewards/q-value), epsilon (whether choose random action), number of iterations, etc. Modify state/action space page: modify the state space (player hp, boss hp, cards on hands\u2026) and action space (possible cards) for the AI training process Train button: run the training process along with a progress bar to indicate how much of training is complete. Unity game visualizer: for manual playtesting and visualizing AI decision making after AI training is complete. Statistics Sheets: after training is complete, user can see an excel sheet that give insights about training performance and AI decision making with respect to the cards in deck. Here is a screenshot of one of the slides for our halves presentation that shows these different components in actions: Integration of AI into Unity GUI An important milestone for our project was the integration of AI into the Unity GUI. This is a big thing for us because now we can finally visualize how the AI sees the cards in any particular state. Below is a screenshot of how this looks right now: The values seen above the cards in the picture above are the expected rewards for playing each card as predicted by the AI. The highest value (in the example above this is Strike Plus) is highlighted to show that this is the card recommended by the AI. If the AI was playing the game, it would always play the highlighted card. Backend integration The system currently works only after the AI has been trained completely. Backend provides the current gamestate to the AI Module, and then gets rewards for each card. Because all projects(AI/Gameplay/Unity GUI) share the same definition of \u2018game state\u2019, this part is simple. Frontend integration As we discussed in previous weeks, frontend is designed to work in a \u2018data-driven\u2019 render mode. It works like a browser; backend gives markups of game state and game event to frontend, and frontend parses the information to render the GUI. So in the frontend, showing reward values is just adding another type of markup. Our markup system and existing rendering workflow is designed to be very flexible with these kind of changes. Runtime Game State Modifier This week we also implemented a runtime game state modifier to allow us modify game data when the game is running. This serves as a tool for programmers to create test cases to test the AI\u2019s decision making. It can also be used by designers to craft scenarios in order to see how the AI interprets certain cards. Here is a screenshot of how this looks right now: Runtime modifier program design When we consider a runtime modifier, we mainly consider 3 things; Flexible/Self-adaptive: We need to modify the gamestate, which keeps changing throughout the development process. The ideal case is this modifier adapts with newer versions of gamestates and we don\u2019t need to rewrite any part of the code. Instant feedback: This is the reason why we want to integrate this in runtime. Because gamestate modifications may happen with a very high frequency (especially when you have a value slider, it will change multiples time in one second). If it reloads the whole game ,which is more than 30 seconds, it would be a very frustrating experience. Undo/redo support: This is a basic requirement of any effective and efficient tool! Approach 1- Incremental modification: A possible design for this is that it works like a version control software like git. We use increments to present each modification and apply these increments to the gamestate. Pros: Instant feedback and very efficient. We can even let modifications happen immediately in the local GUI and then let the backend validate. Cons: Not flexible: we need to write codes for every kind of modification, add/remove/change for all types of data in the gamestate. Undo/redo problem: it is doable, but will take a lot of work. Whenever we write an \u2018Apply()\u2019 function, we also need to write a \u2018Revert()\u2019 function. Approach 2-Snapshot-style modification This is the design we settled on . Whenever modifications happen, it will trigger Frontend, take a snapshot of the current gamestate, and then send back to the python backend to overwrite the current gamestate with the snapshot. Pros: Self-adaptive: We already had gamestate markups encoded by backend, we just modify the markup and send it back to python. Because all encode/decode happens in python, which is a dynamic language. Hence, all changes can be self-adaptive. Easy undo/redo support: We will do this in future. But if it just saves the snapshots in buffer, this will not be too hard. Cons: Instant feedback: this might be a problem in other games, when you want to take a snapshot of the whole game. For now, a snapshot of our game is less than 4000 Bytes, so it is acceptable. Parallel Frontend-Backend Communication In order to better support the runtime modifier, we modified our frontend-backend communication to go from ordered single sequence into multiple parallel sequences. Previous ordered single sequence: In the early stage, the scenario is very simple. GUI sends user input as a request, and python sends markups or system errors as response. The request/response mainloop just follows the gameplay: playerturn, playcard, enemyturn and enemy intent. Current parallel multiple sequence: Now GUI sends more requests than just the player input. It also requests database queries and runtime modifications. In the single sequence model, if the mainloop expects a player input, but receive a database query, it\u2019s very difficult to handle. Now the backend has independent mainloops for gameplay, database queries and runtime modifications. We largely refactored the backend and gameplay code to make this change. Halves Presentation All the above mention progress aside, we were also working on our halves presentation. This is going to be a good opportunity for us to show the ETC faculty about what we have been doing and our journey to achieve the results that we have. Our presentation is on Monday (19-Oct-20) at 4pm and we are looking forward to it. As it turns out, we will be the first team doing halves presentation this semester. We hope to do a good job!","title":"Week 7 : A Little Bit of Success"},{"location":"blogs/week7/#week-7-a-little-bit-of-success","text":"This was the week before halves and all of us spent a considerable amount of effort in preparing for it. However, we also made some considerable progress in both the AI training as well as the Unity visualizer. Most importantly, we managed to improve the AI\u2019s winrate to ~60% which is significantly higher than the previous week\u2019s ~13%. This came as a result of several bug fixes, addition of a new input feature and a modification while calculating q-target values. As part of the Unity visualizer, we can now integrate a trained AI with it to see how the AI predict\u2019s expected rewards from playing cards in a turn. Along with this, we also implemented a state editor tool which allows us to create our own states within Unity that can be very useful in seeing the AI\u2019s reactions in particular states. Lets go over each of these one by one.","title":"Week 7 : A Little Bit of Success"},{"location":"blogs/week7/#ai-algorithm-updates","text":"The list of things we tried and tested this week are as follows:","title":"AI Algorithm Updates"},{"location":"blogs/week7/#addition-of-remaining-damage-for-boss-mode-change-as-an-input-feature","text":"The Guadian (boss in our implementation) in Slay the Spire has two distinct modes offensive and defensive. It also has specific intents for each phase. When the Guardian is in offensive mode, it generally deals a higher damage than when it is in defensive mode. The switch from offensive to defensive modes happens as a result of damage from the player and generally this is a tool for the player to force the boss into the stance which is favorable to the player. This feature had been left out in our initial models which could have been one of the contributing factors to why the AI was performing poorly. Luckily our faculty instructor Mike Christel pointed this out during one of our faculty meetings where he saw the state space and found this value missing. Addition of this feature to our state space did not immediately result in improved results, however we believe that it was a contributing factor that propelled the winrate higher.","title":"Addition of \u2018Remaining Damage for Boss Mode Change\u2019 as an Input Feature"},{"location":"blogs/week7/#action-masking","text":"An approach we focused on this week came as a suggestion from a conversation with a PhD student on main campus James Cunningham. As another strategy to teach the agent about unplayable cards quickly, we decided not only to adjust the q-value of the card that was played but also to adjust the q-values of all the unplayable cards. How do we calculate the target q-values for unplayable cards? Well, since these cards are unplayable we should give them the same negative reward that we give the agent when it tried to play an unplayable card. This means that there would be a negative reward associated with all unplayable cards while constructing the q-target vector only the playable cards could have positive values. Since the largest negative reward is for trying to play unplayable cards and losing, the agent is motivated to quickly learn which cards are unplayable. Earlier, we saw that the agent generally took ~1000-2000 games to learn to not play unplayable cards. With action masking implemented, it now takes ~200-300 to learn to not play unplayable cards. This is a big leap and it gives the agent more time to learn and explore the actual game.","title":"Action Masking"},{"location":"blogs/week7/#testing-different-values-for-number-of-iterations-before-q-models-switch","text":"Last week we introduced the idea of using incremental gamma values along with 2 q-models for our training process. This week we tried an experiment to check what is a good number of iterations for the q-train model to train for before we make the q-model switch. We tested with 10,000 iterations before switching and this resulted in a very poor winrate of ~10%. This is significantly lower than the ~55% winrate that we got with 100,000+ iterations. We also tested with 500,000 iterations and this also gave us a winrate of ~60% implying that increasing the number of iterations too much does not improve the winrate considerably.","title":"Testing different values for Number of Iterations before Q-models switch"},{"location":"blogs/week7/#ideation-towards-our-playable-prototype","text":"One of our deliverables for the project is to create a playable prototype that is accessible for designers, can run AI trainings and can also provide an interface to load up the Unity visualizer. Our current plan is to have a home page which is similar to a central hub that controls everything else. This would contain paths to the following: Create & edit cards page: enable designers to create and edit cards. Deck building page: configure what and how many cards to include in a version of the deck. Modify AI training parameters page: set hyperparameters like gamma (discounting factor when calculating rewards/q-value), epsilon (whether choose random action), number of iterations, etc. Modify state/action space page: modify the state space (player hp, boss hp, cards on hands\u2026) and action space (possible cards) for the AI training process Train button: run the training process along with a progress bar to indicate how much of training is complete. Unity game visualizer: for manual playtesting and visualizing AI decision making after AI training is complete. Statistics Sheets: after training is complete, user can see an excel sheet that give insights about training performance and AI decision making with respect to the cards in deck. Here is a screenshot of one of the slides for our halves presentation that shows these different components in actions:","title":"Ideation towards our Playable Prototype"},{"location":"blogs/week7/#integration-of-ai-into-unity-gui","text":"An important milestone for our project was the integration of AI into the Unity GUI. This is a big thing for us because now we can finally visualize how the AI sees the cards in any particular state. Below is a screenshot of how this looks right now: The values seen above the cards in the picture above are the expected rewards for playing each card as predicted by the AI. The highest value (in the example above this is Strike Plus) is highlighted to show that this is the card recommended by the AI. If the AI was playing the game, it would always play the highlighted card.","title":"Integration of AI into Unity GUI"},{"location":"blogs/week7/#backend-integration","text":"The system currently works only after the AI has been trained completely. Backend provides the current gamestate to the AI Module, and then gets rewards for each card. Because all projects(AI/Gameplay/Unity GUI) share the same definition of \u2018game state\u2019, this part is simple.","title":"Backend integration"},{"location":"blogs/week7/#frontend-integration","text":"As we discussed in previous weeks, frontend is designed to work in a \u2018data-driven\u2019 render mode. It works like a browser; backend gives markups of game state and game event to frontend, and frontend parses the information to render the GUI. So in the frontend, showing reward values is just adding another type of markup. Our markup system and existing rendering workflow is designed to be very flexible with these kind of changes.","title":"Frontend integration"},{"location":"blogs/week7/#runtime-game-state-modifier","text":"This week we also implemented a runtime game state modifier to allow us modify game data when the game is running. This serves as a tool for programmers to create test cases to test the AI\u2019s decision making. It can also be used by designers to craft scenarios in order to see how the AI interprets certain cards. Here is a screenshot of how this looks right now:","title":"Runtime Game State Modifier"},{"location":"blogs/week7/#runtime-modifier-program-design","text":"When we consider a runtime modifier, we mainly consider 3 things; Flexible/Self-adaptive: We need to modify the gamestate, which keeps changing throughout the development process. The ideal case is this modifier adapts with newer versions of gamestates and we don\u2019t need to rewrite any part of the code. Instant feedback: This is the reason why we want to integrate this in runtime. Because gamestate modifications may happen with a very high frequency (especially when you have a value slider, it will change multiples time in one second). If it reloads the whole game ,which is more than 30 seconds, it would be a very frustrating experience. Undo/redo support: This is a basic requirement of any effective and efficient tool!","title":"Runtime modifier program design"},{"location":"blogs/week7/#approach-1-incremental-modification","text":"A possible design for this is that it works like a version control software like git. We use increments to present each modification and apply these increments to the gamestate. Pros: Instant feedback and very efficient. We can even let modifications happen immediately in the local GUI and then let the backend validate. Cons: Not flexible: we need to write codes for every kind of modification, add/remove/change for all types of data in the gamestate. Undo/redo problem: it is doable, but will take a lot of work. Whenever we write an \u2018Apply()\u2019 function, we also need to write a \u2018Revert()\u2019 function.","title":"Approach 1- Incremental modification:"},{"location":"blogs/week7/#approach-2-snapshot-style-modification","text":"This is the design we settled on . Whenever modifications happen, it will trigger Frontend, take a snapshot of the current gamestate, and then send back to the python backend to overwrite the current gamestate with the snapshot. Pros: Self-adaptive: We already had gamestate markups encoded by backend, we just modify the markup and send it back to python. Because all encode/decode happens in python, which is a dynamic language. Hence, all changes can be self-adaptive. Easy undo/redo support: We will do this in future. But if it just saves the snapshots in buffer, this will not be too hard. Cons: Instant feedback: this might be a problem in other games, when you want to take a snapshot of the whole game. For now, a snapshot of our game is less than 4000 Bytes, so it is acceptable.","title":"Approach 2-Snapshot-style modification"},{"location":"blogs/week7/#parallel-frontend-backend-communication","text":"In order to better support the runtime modifier, we modified our frontend-backend communication to go from ordered single sequence into multiple parallel sequences. Previous ordered single sequence: In the early stage, the scenario is very simple. GUI sends user input as a request, and python sends markups or system errors as response. The request/response mainloop just follows the gameplay: playerturn, playcard, enemyturn and enemy intent. Current parallel multiple sequence: Now GUI sends more requests than just the player input. It also requests database queries and runtime modifications. In the single sequence model, if the mainloop expects a player input, but receive a database query, it\u2019s very difficult to handle. Now the backend has independent mainloops for gameplay, database queries and runtime modifications. We largely refactored the backend and gameplay code to make this change.","title":"Parallel Frontend-Backend Communication"},{"location":"blogs/week7/#halves-presentation","text":"All the above mention progress aside, we were also working on our halves presentation. This is going to be a good opportunity for us to show the ETC faculty about what we have been doing and our journey to achieve the results that we have. Our presentation is on Monday (19-Oct-20) at 4pm and we are looking forward to it. As it turns out, we will be the first team doing halves presentation this semester. We hope to do a good job!","title":"Halves Presentation"},{"location":"blogs/week8/","text":"Week 8 : An App Awakens The biggest event this week, of course, was the halves presentation on Monday. It went pretty well and we got some nice feedback from the faculty later. The progress this week was a little slow on the AI. On the other hand, we have started worked towards an all-inclusive app that will combine all the different pieces that we have been working on. The team also had meetings with some game design faculty this week to try and understand what are the things game designers would potentially look for in a tool like the one we are making. Halves Feedback Here are some avenues we can improve on considering the feedback we received: We could have been more clear to show how the AI is improving over iterations It was tough to get a clear sense of what we can learn from the AI expert Comparison to other machine learning techniques can fetch us some comparative data on how the agent is performing We need some sort of external validation (perhaps from ML faculty)\\ We need some playtesting with designers We agree with all of these points. Over the last few weeks, it has become increasingly clear to us that we need to shift out approach to ensure that we appropriately serve game designers. Up until this point, the focus of our product has been on the technical side. This was logical because without a sound technical foundation, we have no product. However, now that we have an AI that learns to perform decently well (~70% winrate) which enables us to start thinking about serving designers. External validation is a highlight from this feedback. We need external validation from ML faculty as well as from game designers about how easy it is to use our product. Although our winrate has improved significantly, we believe that there is still room for improvement. Going the last mile is going to require effort and a deeper dive into potential machine learning techniques. Validation from ML faculty could help with this. On the other hand, validation from game designers is absolutely crucial because at the end of the day, this tool is being built to serve them. AI Updates This week did not see any big jumps in the AI performance. Although, we have managed to improve winrate by around 10%. Last week, we had achieved a winrate of ~61%. This week, we have improved a little and are achieving a winrate of ~72-73%. The technique behind this change involved getting rid of the elaborate reward function we had earlier. This reward function was designed in a way that each card had its own individual logic for calculating reward. The reward function for the card Flex for example, would look at all the attack cards played after Flex in a given turn and try to calculate how much extra damage was done because of Flex and then use that to calculate the reward attributed to Flex. The biggest problem with this technique was that it was not scalable since each card with a buff required a custom reward function. The new technique is to attribute reward values only for winning or losing the game. There is no intermediate reward calculated for each individual turn. In contrast to the previously used technique, this approach is easily scalable. At the same time, it is also a more intuitive and logical approach since a reinforcement learning agent should only be rewarded for achieving the goal. With the introduction of intermediary reward, we introduce bias into the agent depending on what the human programming the agent thinks is the correct approach. The other important issue for the week was to focus on figuring out why we see constant drops in the reward value in the training graphs. An example of this can be seen below: As you can see from this image, there are consistent and periodic drops in the reward values. This graph shows rolling averages of 20 games. This means that the drop is not limited to a single game but a series of games. We see that as the number of iterations increase, the distance between drops is going down. This might be a clue indicating that as the AI\u2019s age might have something to do with the drops. However, it is still a mystery as to why this is happening. We initially assumed that the natural culprit for the causing the drops would be Q-model switches. However, there is no logical reason to believe that the Q-model switch is in fact causing this. Not to mention, there are a significantly higher number of drops than Q-model switches. For the graph seen above, there have only been 5 Q-model switches whereas the number of drops is 21. The other natural culprit for this can be the data collector or the data writer. Perhaps there is nothing wrong with the algorithm instead there is something wrong with the collection and interpretation of this data. After all, we know that the winrate is true because we have tested the trained AI agent multiple times and it is clear that it has learned a lot about the game. Going into the following week, we will still be looking closely at the AI training to figure this out. We theorize that solving this problem may hold the key to further improve our winrate. Gameplay Rebuilding An important focus of this week has also been to include the remaining portion of the Slay the Spire game that we had initially planned. Right now, there are many Ironclad cards and buffs that are not a part of our game. With the AI working well, we are heading over to include this remaining part in the game. Motivations Extendibility: We want to add more cards, more buffs, more mechanisms into our game to let the AI play. We need to make some changes on the current game code in order to restructure it so that we can add new features easily. Generalize our method: We want to allow the user to customize the game not only on the game data level, but also deep into the gameplay logic level. This, however, should still let AI play the game. How it works Previously: In the older structure, logic of specific cards was embedded in the gameplay logic. This is convenient. But when we try to add new buffs and new card mechanisms, we need to look at the intertwined gameplay modules to figure out how to make these changes. An illustration of this can be found below: Current: Now there are two parts in our gameplay. One is gameplay core, which provides the basic logic and game mainloop. The other one is gameplay extension, which includes the customized gameplay logic. The gameplay core provides API , and gameplay extensions use these API. During the runtime, it will dynamically load the gameplay extension code. To add new mechanism(buffs/card effect), we just simply follow the API, and don\u2019t need change the gameplay core or worry about the details how they are implemented. Below is an illustration of how this would work: One App for All We plan to develop one app for everything, including AI Module, Unity frontend, python gameplay, card/deck editing, data visualization, etc. A screenshot of our first prototype is shown below: Motivation Accessible to designers: After halves, we started to think about how to serve designers. We need a user friendly GUI to let designers use all the tools we provide. Organize our tools chain: We have many tools built on different platforms using different techniques. We even plan to build more tools. This is a good chance for us to bring everything together in an organized and set a precedent for what comes next. Consideration of Different Approaches System Built in Unity: Pros: Our game\u2019s GUI is developed in Unity, and we are familiar with Unity. Cons: Unity\u2019s UI system is not designed for a generalized desktop GUI. This would be a big problem when we try to build complex UI such as data visualization. Browser Based : HTML, CSS, JavaScript Pros: Easy to use and easy to develop. Web techniques are convenient and have many libraries and frameworks. Cons: Browsers usually don\u2019t support manipulating local files and start executables, which is very important for us. Electron (Finally picked this) We ended up choosing electron because of two main reasons: Uses web standards: Developing a desktop app is basically the same as writing a website in electron. It uses html, javascript and css, and we are already familiar with these techniques. Highlevel, lightweight: Because of html and javascript, developing an app is much easier than other techniques such as windows naive API, Qt, etc. An Introduction to Electron Electron is a framework for creating native applications with web technologies like JavaScript, HTML, and CSS. It takes care of the hard parts so we can focus on the core of our application. Here is a link to its website \u2013 https://www.electronjs.org/ We want to build cross-platform desktop apps for designers to use. Functions like create and edit cards, deck building, running training scripts, and view replay in unity can be activated from a central control panel created by Electron. Below is a screenshot of what a homescreen containing this would look like: Using Electron to Edit Cards This part of the application was implemented this week. The motivation for doing this is to make it easier for designers to modify cards. We do not want them to open json files and edit values there since this may be a little daunting for those who are not familiar with json files. Instead, now the user can edit a card file and save it using our tool. Insights from Meetings with Game Design Faculty This week we met with Jessica Hammer and Dave Culyba to talk about the different things a game designer would be looking for in our tool. Here is some interesting insights that we got: What are the things that an AI playtester can give you but a human playtester cannot? Try to incorporate as many of these things in your tool as possible. There are many things that an AI can quantify but a human cannot. Focus on outliers. Highlight and save game trajectories where something out of the ordinary happens. Try to figure out how these trajectories came to be. How to average reward values of cards change over iterations during training. This might give important information about how the AI is learning. There must be a level of certainty about whether the AI results are true and trustworthy. Is the AI learning similar gameplay as compared to a human. Getting in touch with the Slay the Spire team can help us with this. Along with these insights, we also got a few suggestions of things we should try and do which we are currently looking into: Prepare a list of different statistical values/trends that can be generated using AI Playtesting. Do this as a brainstorming session. Write out / role play the conversation a designer would have while looking at our tool. What would go through their heads when interacting with this tool? What are the different questions they would be trying to answer with our tool? Despite this being the week with halves, we managed to get a lot of things done in a variety of areas. The project is looking more and more promising as each week passes. We are excited about what the future holds in store for us!","title":"Week 8 : An App Awakens"},{"location":"blogs/week8/#week-8-an-app-awakens","text":"The biggest event this week, of course, was the halves presentation on Monday. It went pretty well and we got some nice feedback from the faculty later. The progress this week was a little slow on the AI. On the other hand, we have started worked towards an all-inclusive app that will combine all the different pieces that we have been working on. The team also had meetings with some game design faculty this week to try and understand what are the things game designers would potentially look for in a tool like the one we are making.","title":"Week 8 : An App Awakens"},{"location":"blogs/week8/#halves-feedback","text":"Here are some avenues we can improve on considering the feedback we received: We could have been more clear to show how the AI is improving over iterations It was tough to get a clear sense of what we can learn from the AI expert Comparison to other machine learning techniques can fetch us some comparative data on how the agent is performing We need some sort of external validation (perhaps from ML faculty)\\ We need some playtesting with designers We agree with all of these points. Over the last few weeks, it has become increasingly clear to us that we need to shift out approach to ensure that we appropriately serve game designers. Up until this point, the focus of our product has been on the technical side. This was logical because without a sound technical foundation, we have no product. However, now that we have an AI that learns to perform decently well (~70% winrate) which enables us to start thinking about serving designers. External validation is a highlight from this feedback. We need external validation from ML faculty as well as from game designers about how easy it is to use our product. Although our winrate has improved significantly, we believe that there is still room for improvement. Going the last mile is going to require effort and a deeper dive into potential machine learning techniques. Validation from ML faculty could help with this. On the other hand, validation from game designers is absolutely crucial because at the end of the day, this tool is being built to serve them.","title":"Halves Feedback"},{"location":"blogs/week8/#ai-updates","text":"This week did not see any big jumps in the AI performance. Although, we have managed to improve winrate by around 10%. Last week, we had achieved a winrate of ~61%. This week, we have improved a little and are achieving a winrate of ~72-73%. The technique behind this change involved getting rid of the elaborate reward function we had earlier. This reward function was designed in a way that each card had its own individual logic for calculating reward. The reward function for the card Flex for example, would look at all the attack cards played after Flex in a given turn and try to calculate how much extra damage was done because of Flex and then use that to calculate the reward attributed to Flex. The biggest problem with this technique was that it was not scalable since each card with a buff required a custom reward function. The new technique is to attribute reward values only for winning or losing the game. There is no intermediate reward calculated for each individual turn. In contrast to the previously used technique, this approach is easily scalable. At the same time, it is also a more intuitive and logical approach since a reinforcement learning agent should only be rewarded for achieving the goal. With the introduction of intermediary reward, we introduce bias into the agent depending on what the human programming the agent thinks is the correct approach. The other important issue for the week was to focus on figuring out why we see constant drops in the reward value in the training graphs. An example of this can be seen below: As you can see from this image, there are consistent and periodic drops in the reward values. This graph shows rolling averages of 20 games. This means that the drop is not limited to a single game but a series of games. We see that as the number of iterations increase, the distance between drops is going down. This might be a clue indicating that as the AI\u2019s age might have something to do with the drops. However, it is still a mystery as to why this is happening. We initially assumed that the natural culprit for the causing the drops would be Q-model switches. However, there is no logical reason to believe that the Q-model switch is in fact causing this. Not to mention, there are a significantly higher number of drops than Q-model switches. For the graph seen above, there have only been 5 Q-model switches whereas the number of drops is 21. The other natural culprit for this can be the data collector or the data writer. Perhaps there is nothing wrong with the algorithm instead there is something wrong with the collection and interpretation of this data. After all, we know that the winrate is true because we have tested the trained AI agent multiple times and it is clear that it has learned a lot about the game. Going into the following week, we will still be looking closely at the AI training to figure this out. We theorize that solving this problem may hold the key to further improve our winrate.","title":"AI Updates"},{"location":"blogs/week8/#gameplay-rebuilding","text":"An important focus of this week has also been to include the remaining portion of the Slay the Spire game that we had initially planned. Right now, there are many Ironclad cards and buffs that are not a part of our game. With the AI working well, we are heading over to include this remaining part in the game.","title":"Gameplay Rebuilding"},{"location":"blogs/week8/#motivations","text":"Extendibility: We want to add more cards, more buffs, more mechanisms into our game to let the AI play. We need to make some changes on the current game code in order to restructure it so that we can add new features easily. Generalize our method: We want to allow the user to customize the game not only on the game data level, but also deep into the gameplay logic level. This, however, should still let AI play the game.","title":"Motivations"},{"location":"blogs/week8/#how-it-works","text":"Previously: In the older structure, logic of specific cards was embedded in the gameplay logic. This is convenient. But when we try to add new buffs and new card mechanisms, we need to look at the intertwined gameplay modules to figure out how to make these changes. An illustration of this can be found below: Current: Now there are two parts in our gameplay. One is gameplay core, which provides the basic logic and game mainloop. The other one is gameplay extension, which includes the customized gameplay logic. The gameplay core provides API , and gameplay extensions use these API. During the runtime, it will dynamically load the gameplay extension code. To add new mechanism(buffs/card effect), we just simply follow the API, and don\u2019t need change the gameplay core or worry about the details how they are implemented. Below is an illustration of how this would work:","title":"How it works"},{"location":"blogs/week8/#one-app-for-all","text":"We plan to develop one app for everything, including AI Module, Unity frontend, python gameplay, card/deck editing, data visualization, etc. A screenshot of our first prototype is shown below:","title":"One App for All"},{"location":"blogs/week8/#motivation","text":"Accessible to designers: After halves, we started to think about how to serve designers. We need a user friendly GUI to let designers use all the tools we provide. Organize our tools chain: We have many tools built on different platforms using different techniques. We even plan to build more tools. This is a good chance for us to bring everything together in an organized and set a precedent for what comes next.","title":"Motivation"},{"location":"blogs/week8/#consideration-of-different-approaches","text":"System Built in Unity: Pros: Our game\u2019s GUI is developed in Unity, and we are familiar with Unity. Cons: Unity\u2019s UI system is not designed for a generalized desktop GUI. This would be a big problem when we try to build complex UI such as data visualization. Browser Based : HTML, CSS, JavaScript Pros: Easy to use and easy to develop. Web techniques are convenient and have many libraries and frameworks. Cons: Browsers usually don\u2019t support manipulating local files and start executables, which is very important for us. Electron (Finally picked this) We ended up choosing electron because of two main reasons: Uses web standards: Developing a desktop app is basically the same as writing a website in electron. It uses html, javascript and css, and we are already familiar with these techniques. Highlevel, lightweight: Because of html and javascript, developing an app is much easier than other techniques such as windows naive API, Qt, etc.","title":"Consideration of Different Approaches"},{"location":"blogs/week8/#an-introduction-to-electron","text":"Electron is a framework for creating native applications with web technologies like JavaScript, HTML, and CSS. It takes care of the hard parts so we can focus on the core of our application. Here is a link to its website \u2013 https://www.electronjs.org/ We want to build cross-platform desktop apps for designers to use. Functions like create and edit cards, deck building, running training scripts, and view replay in unity can be activated from a central control panel created by Electron. Below is a screenshot of what a homescreen containing this would look like:","title":"An Introduction to Electron"},{"location":"blogs/week8/#using-electron-to-edit-cards","text":"This part of the application was implemented this week. The motivation for doing this is to make it easier for designers to modify cards. We do not want them to open json files and edit values there since this may be a little daunting for those who are not familiar with json files. Instead, now the user can edit a card file and save it using our tool.","title":"Using Electron to Edit Cards"},{"location":"blogs/week8/#insights-from-meetings-with-game-design-faculty","text":"This week we met with Jessica Hammer and Dave Culyba to talk about the different things a game designer would be looking for in our tool. Here is some interesting insights that we got: What are the things that an AI playtester can give you but a human playtester cannot? Try to incorporate as many of these things in your tool as possible. There are many things that an AI can quantify but a human cannot. Focus on outliers. Highlight and save game trajectories where something out of the ordinary happens. Try to figure out how these trajectories came to be. How to average reward values of cards change over iterations during training. This might give important information about how the AI is learning. There must be a level of certainty about whether the AI results are true and trustworthy. Is the AI learning similar gameplay as compared to a human. Getting in touch with the Slay the Spire team can help us with this. Along with these insights, we also got a few suggestions of things we should try and do which we are currently looking into: Prepare a list of different statistical values/trends that can be generated using AI Playtesting. Do this as a brainstorming session. Write out / role play the conversation a designer would have while looking at our tool. What would go through their heads when interacting with this tool? What are the different questions they would be trying to answer with our tool? Despite this being the week with halves, we managed to get a lot of things done in a variety of areas. The project is looking more and more promising as each week passes. We are excited about what the future holds in store for us!","title":"Insights from Meetings with Game Design Faculty"},{"location":"blogs/week9/","text":"Week 9 : Designers First As is evident from the title of the blog, this week involved taking more steps towards making our tool more accessible to designers. There were a few AI experiments and bug fixes. However, we did not see any improvements in the winrate (currently at ~70% for our base deck). The real progress this week came from the app building end which focused on making our tool more friendly for designers. This was our main focus last week and has continued on for this week as well. Currently, we are tirelessly working towards building a single application for playtesting it with designers on Nov9-Nov10 . Needless to say, this will be the focus of our work for this and next week as well. AI Updates This week consisted of several experiments without yielding any significant results. There were also a couple of bug fixes which help present data more accurately in the training excel sheet. Recurring Reward Drop Bug Fix Continuing from the previous week, the focus on the AI front was to figure out why the reward was dropping periodically during AI training. An example of this from last week can be seen below. The theory we initially came up with was that something was going wrong which caused the agent to \u2018forget\u2019 everything it had learnt and start again from scratch. However, the actual reason did not have much to do with the agent. In fact, there was a bug in the data collector script which is responsible for collecting and storing data from the training process and then writes it to the excel sheet. After fixing the bug, the training graph now looks something like this: This graph now looks much more like a reinforcement learning reward graph with steep rise in the average reward at the start and then becoming stationary over time (with some noise). Start with High Discounting Factor (Gamma) One of the things we tested this week was to keep everything else the same but start with a high gamma value. The reason we moved to an incremental gamma earlier was because the model would hardly learn anything and would converge very early to the reward value of playing an unplayable card. Meaning, it would try to keep playing unplayable cards and keep losing. So why did we try this again. We wanted to lay more emphasis on future rewards because it could be the key to further improve our winrate. Also, now that we had a model which could give stable results, we figured we could go back to the earlier method and see if we can make it working with help of our newer model. Unfortunately, the model suffers from the same problem it did before. The model never even crosses the first step of learning which are the playable cards in the current turn and playing one of those. We also tried bootstrapping the model using a trained model which had a ~70% winrate. Even this did not really change anything. The model still was not able to learn how to play playable cards. We think the problem here is that with a high gamma, what can happen in the next state take precedence over the current state and causing an unrealistically high q-value for unplayable cards. Desktop App \u2013 Design We now come to the desktop app design which is where a lot of effort was focused this week. Last week we made a decision to use electron to build a desktop application which includes all tools. We had just started to learn how to use electron. This week we have made solid progress on several key features of this app. The following are some of the features we implemented this week: Environment detection: Our project relies on a specific version of python and tensorflow. Thus, we need a mechanism to ensure that the user has the environment to run our application properly. We are now able to detect the environment when the application starts. Next step is to help install modules of the environment if the user doesn\u2019t have them. Game Project management: As designed before, our app can have multiple games with different cards, rules and decks. Hence we need to allow the user to view existing game projects and remove and create new game projects by just clicking buttons. Quick view and shortcut operations: We implemented this \u201cquick view\u201d section on the game\u2019s page (picture below). The purpose of this section is to let users have a basic understanding of what games look like( what are cards in the deck, which deck am I using, etc). Another goal is to facilitate shortcuts operations like switching to another deck. Main operation of a game project: We have several operations we need to build for: play the game, design the game, playtesting or train the AI. This week , we made \u201cplay\u201d and \u201cremove\u201d operations work. When you click \u201cplay\u201d, electron will launch the Unity app to load the game you select. Desktop App \u2013 Components Our app is a single place for many different aspects of playtesting with AI that is brought together as a unified system. Here are some of the key components that we worked towards this week: Data visualization Data visualization might be the most important part of this app. However, drawing graphics, especially interactive ones, is quite difficult and could prove to be very challenging. NAIVE/LOW-LEVEL SOLUTIONS One of our discussed solutions was to draw data graphics by using svg, since html standards support this format. We could even use a library like WebGL. Even though this allows us to have highly customized graphics to show our data, building a data visualization module from scratch is out of our scope and involves a lot of unnecessary work. DATA VISUALIZATION LIBRARY We then turned to Javascript, which has many data visualization libraries. We finally choose d3.js (https://github.com/d3/d3 ) among others for 2 reasons: Independent: Many other data visualization libraries( such as Victory, Rechard,etc) only work with some front-end frameworks like React and Vue. d3.js is easier to integrate since it only needs javascript Community resoruces\uff1a We found that there were abundant templates and tutorials of d3.js on the internet. Database There is a lot of data that can be manipulated using this app. This week we also wrote a database module in javascript to manage that. We had 2 options for this: PYTHON DATABASE MODULE + RPCS: We already have a database module implemented in python. Javascript uses a RPC mechanism to call python to get the query result. This is nice because the code is consistent and doesn\u2019t have duplications. However, building RPCs, even with 3rd party libraries, is highly time consuming. JAVASCRIPT DATABASE MODULE (OUR CHOICE): We decided to write a javascript database module because we found this to be the quickest. Even though there are duplicated code both in python and javascript, it is under control because over 70% of the functions of the current database are all about reading and parsing static files. As long as we make the rules of static files\u2019 paths and names consistent, javascript and python can all follow these rules. Multi-processes design A lot of the core logic of our app is written in different languages. For example, we use Unity to build game GUI but AI and gameplay are implemented in python. So our desktop app has to be a multi-processes one. Electron has good support for managing child processes. As for process communication, we are still using sockets because we already have this codebase on the python side. Below is a relationship of the different processes: Why do we separate the python gameplay process with other python processes? : The nature of inter-process communication is different. Communication between unity GUI and python gameplay is much more complicated than others. The protocol between them is a comprehensive client-server style. But the communication between electron and python is RPC style: just call functions and get results. Another reason is that historically in our project, communication between Unity and python gameplay is highly completed. There is no reason for electron to act as a middle man passing messages between them. Card Editing and Deck Building This week our team discussed card and deck editing functions. The two functions were separated into two different pages, but we improved our design by creating a dashboard to visualize the current player\u2019s cards and deck at the same page. The idea we got referenced the popular card game by Blizzard called Hearthstone. We want our card/deck building system to look similar to the screenshot below from Hearthstone: The picture above is a screenshot of deck building in Hearthstone Inspired from this, the following are the function specifications that we are implementing. Dashboard page: This page shows existing cards and deck information in a particular game version. By modifying card and deck data, users can build a custom deck, trying their own combination of cards to playtest a game version. Here are some functions that we have built for this system: Card: Add a new card: go to a page where users can fill in information and create new cards. (done) Edit card: go to a page where users can modify information for existing cards. (done) Delete: click to delete card. Add to Deck: add this card to deck. Search: search by name. Filter: filter by energy or other custom criteria. View card information: hover on card picture to show description. Deck: Save: save changes being made by the user. (done) Indicator: Changes being made will be highlighted in light blue background. Add new Deck: add new deck. Undo A screenshot of our current dashboard page can be found below: Edit Card page: The purpose of this page is to create custom new cards and modify existing cards. Select bar: Select bar to jump to another card editor. (done) List information: List all information required to create a card. (done) Save: save changes being made by user. (done) Indicator: Changes being made will be highlighted in light blue background. (done) Sanity check: input number should be in the range between 0 to 100. (done) Add buff: Add existing buffs to a card. Image upload: click to upload custom image. Screenshot of the Edit Card page is shown below: UI Design Based on the design and architecture described above, we did some UI design for what our app should finally look like. Below are screenshots and description of the same. Home Page Left Navigation bar is similar to the Epic Games Launcher. The functions on the sidebar include: Home: Home page, our website, resources, etc. Library: Game app, you can create different game apps here. Cards: Create cards in a pre-selected game app. Decks: Create decks in a pre-selected game app. Training: Start a training program in a pre-selected game app and card deck. Library Page Left tabs shows a list of all the current game versions. Description: Description of this game version. Rules: Rule set of this game version. Deck Overview: An overview of all the decks and the option to select one. Use this App: Select a game version Remove: Delete this game version. Play: Play this version of the game in Unity. Cards Page A page that shows all cards. CREATE NEW CARD EDIT EXISTING CARD Deck Page A page that shows all the deck in the current game version. DECK EDITOR All in all, this week saw a lot of progress on building the app. The next week is also going to focus heavily on building the app because of our playtesting deadline. We hope to create something that is simple enough for designers to use!","title":"Week 9 : Designers First"},{"location":"blogs/week9/#week-9-designers-first","text":"As is evident from the title of the blog, this week involved taking more steps towards making our tool more accessible to designers. There were a few AI experiments and bug fixes. However, we did not see any improvements in the winrate (currently at ~70% for our base deck). The real progress this week came from the app building end which focused on making our tool more friendly for designers. This was our main focus last week and has continued on for this week as well. Currently, we are tirelessly working towards building a single application for playtesting it with designers on Nov9-Nov10 . Needless to say, this will be the focus of our work for this and next week as well.","title":"Week 9 : Designers First"},{"location":"blogs/week9/#ai-updates","text":"This week consisted of several experiments without yielding any significant results. There were also a couple of bug fixes which help present data more accurately in the training excel sheet.","title":"AI Updates"},{"location":"blogs/week9/#recurring-reward-drop-bug-fix","text":"Continuing from the previous week, the focus on the AI front was to figure out why the reward was dropping periodically during AI training. An example of this from last week can be seen below. The theory we initially came up with was that something was going wrong which caused the agent to \u2018forget\u2019 everything it had learnt and start again from scratch. However, the actual reason did not have much to do with the agent. In fact, there was a bug in the data collector script which is responsible for collecting and storing data from the training process and then writes it to the excel sheet. After fixing the bug, the training graph now looks something like this: This graph now looks much more like a reinforcement learning reward graph with steep rise in the average reward at the start and then becoming stationary over time (with some noise).","title":"Recurring Reward Drop Bug Fix"},{"location":"blogs/week9/#start-with-high-discounting-factor-gamma","text":"One of the things we tested this week was to keep everything else the same but start with a high gamma value. The reason we moved to an incremental gamma earlier was because the model would hardly learn anything and would converge very early to the reward value of playing an unplayable card. Meaning, it would try to keep playing unplayable cards and keep losing. So why did we try this again. We wanted to lay more emphasis on future rewards because it could be the key to further improve our winrate. Also, now that we had a model which could give stable results, we figured we could go back to the earlier method and see if we can make it working with help of our newer model. Unfortunately, the model suffers from the same problem it did before. The model never even crosses the first step of learning which are the playable cards in the current turn and playing one of those. We also tried bootstrapping the model using a trained model which had a ~70% winrate. Even this did not really change anything. The model still was not able to learn how to play playable cards. We think the problem here is that with a high gamma, what can happen in the next state take precedence over the current state and causing an unrealistically high q-value for unplayable cards.","title":"Start with High Discounting Factor (Gamma)"},{"location":"blogs/week9/#desktop-app-design","text":"We now come to the desktop app design which is where a lot of effort was focused this week. Last week we made a decision to use electron to build a desktop application which includes all tools. We had just started to learn how to use electron. This week we have made solid progress on several key features of this app. The following are some of the features we implemented this week:","title":"Desktop App \u2013 Design"},{"location":"blogs/week9/#environment-detection","text":"Our project relies on a specific version of python and tensorflow. Thus, we need a mechanism to ensure that the user has the environment to run our application properly. We are now able to detect the environment when the application starts. Next step is to help install modules of the environment if the user doesn\u2019t have them.","title":"Environment detection:"},{"location":"blogs/week9/#game-project-management","text":"As designed before, our app can have multiple games with different cards, rules and decks. Hence we need to allow the user to view existing game projects and remove and create new game projects by just clicking buttons.","title":"Game Project management:"},{"location":"blogs/week9/#quick-view-and-shortcut-operations","text":"We implemented this \u201cquick view\u201d section on the game\u2019s page (picture below). The purpose of this section is to let users have a basic understanding of what games look like( what are cards in the deck, which deck am I using, etc). Another goal is to facilitate shortcuts operations like switching to another deck.","title":"Quick view and shortcut operations:"},{"location":"blogs/week9/#main-operation-of-a-game-project","text":"We have several operations we need to build for: play the game, design the game, playtesting or train the AI. This week , we made \u201cplay\u201d and \u201cremove\u201d operations work. When you click \u201cplay\u201d, electron will launch the Unity app to load the game you select.","title":"Main operation of a game project:"},{"location":"blogs/week9/#desktop-app-components","text":"Our app is a single place for many different aspects of playtesting with AI that is brought together as a unified system. Here are some of the key components that we worked towards this week:","title":"Desktop App \u2013 Components"},{"location":"blogs/week9/#data-visualization","text":"Data visualization might be the most important part of this app. However, drawing graphics, especially interactive ones, is quite difficult and could prove to be very challenging.","title":"Data visualization"},{"location":"blogs/week9/#naivelow-level-solutions","text":"One of our discussed solutions was to draw data graphics by using svg, since html standards support this format. We could even use a library like WebGL. Even though this allows us to have highly customized graphics to show our data, building a data visualization module from scratch is out of our scope and involves a lot of unnecessary work.","title":"NAIVE/LOW-LEVEL SOLUTIONS"},{"location":"blogs/week9/#data-visualization-library","text":"We then turned to Javascript, which has many data visualization libraries. We finally choose d3.js (https://github.com/d3/d3 ) among others for 2 reasons: Independent: Many other data visualization libraries( such as Victory, Rechard,etc) only work with some front-end frameworks like React and Vue. d3.js is easier to integrate since it only needs javascript Community resoruces\uff1a We found that there were abundant templates and tutorials of d3.js on the internet.","title":"DATA VISUALIZATION LIBRARY"},{"location":"blogs/week9/#database","text":"There is a lot of data that can be manipulated using this app. This week we also wrote a database module in javascript to manage that. We had 2 options for this:","title":"Database"},{"location":"blogs/week9/#python-database-module-rpcs","text":"We already have a database module implemented in python. Javascript uses a RPC mechanism to call python to get the query result. This is nice because the code is consistent and doesn\u2019t have duplications. However, building RPCs, even with 3rd party libraries, is highly time consuming.","title":"PYTHON DATABASE MODULE + RPCS:"},{"location":"blogs/week9/#javascript-database-module-our-choice","text":"We decided to write a javascript database module because we found this to be the quickest. Even though there are duplicated code both in python and javascript, it is under control because over 70% of the functions of the current database are all about reading and parsing static files. As long as we make the rules of static files\u2019 paths and names consistent, javascript and python can all follow these rules.","title":"JAVASCRIPT DATABASE MODULE (OUR CHOICE):"},{"location":"blogs/week9/#multi-processes-design","text":"A lot of the core logic of our app is written in different languages. For example, we use Unity to build game GUI but AI and gameplay are implemented in python. So our desktop app has to be a multi-processes one. Electron has good support for managing child processes. As for process communication, we are still using sockets because we already have this codebase on the python side. Below is a relationship of the different processes: Why do we separate the python gameplay process with other python processes? : The nature of inter-process communication is different. Communication between unity GUI and python gameplay is much more complicated than others. The protocol between them is a comprehensive client-server style. But the communication between electron and python is RPC style: just call functions and get results. Another reason is that historically in our project, communication between Unity and python gameplay is highly completed. There is no reason for electron to act as a middle man passing messages between them.","title":"Multi-processes design"},{"location":"blogs/week9/#card-editing-and-deck-building","text":"This week our team discussed card and deck editing functions. The two functions were separated into two different pages, but we improved our design by creating a dashboard to visualize the current player\u2019s cards and deck at the same page. The idea we got referenced the popular card game by Blizzard called Hearthstone. We want our card/deck building system to look similar to the screenshot below from Hearthstone: The picture above is a screenshot of deck building in Hearthstone Inspired from this, the following are the function specifications that we are implementing.","title":"Card Editing and Deck Building"},{"location":"blogs/week9/#dashboard-page","text":"This page shows existing cards and deck information in a particular game version. By modifying card and deck data, users can build a custom deck, trying their own combination of cards to playtest a game version. Here are some functions that we have built for this system: Card: Add a new card: go to a page where users can fill in information and create new cards. (done) Edit card: go to a page where users can modify information for existing cards. (done) Delete: click to delete card. Add to Deck: add this card to deck. Search: search by name. Filter: filter by energy or other custom criteria. View card information: hover on card picture to show description. Deck: Save: save changes being made by the user. (done) Indicator: Changes being made will be highlighted in light blue background. Add new Deck: add new deck. Undo A screenshot of our current dashboard page can be found below:","title":"Dashboard page:"},{"location":"blogs/week9/#edit-card-page","text":"The purpose of this page is to create custom new cards and modify existing cards. Select bar: Select bar to jump to another card editor. (done) List information: List all information required to create a card. (done) Save: save changes being made by user. (done) Indicator: Changes being made will be highlighted in light blue background. (done) Sanity check: input number should be in the range between 0 to 100. (done) Add buff: Add existing buffs to a card. Image upload: click to upload custom image. Screenshot of the Edit Card page is shown below:","title":"Edit Card page:"},{"location":"blogs/week9/#ui-design","text":"Based on the design and architecture described above, we did some UI design for what our app should finally look like. Below are screenshots and description of the same.","title":"UI Design"},{"location":"blogs/week9/#home-page","text":"Left Navigation bar is similar to the Epic Games Launcher. The functions on the sidebar include: Home: Home page, our website, resources, etc. Library: Game app, you can create different game apps here. Cards: Create cards in a pre-selected game app. Decks: Create decks in a pre-selected game app. Training: Start a training program in a pre-selected game app and card deck.","title":"Home Page"},{"location":"blogs/week9/#library-page","text":"Left tabs shows a list of all the current game versions. Description: Description of this game version. Rules: Rule set of this game version. Deck Overview: An overview of all the decks and the option to select one. Use this App: Select a game version Remove: Delete this game version. Play: Play this version of the game in Unity.","title":"Library Page"},{"location":"blogs/week9/#cards-page","text":"A page that shows all cards.","title":"Cards Page"},{"location":"blogs/week9/#create-new-card","text":"","title":"CREATE NEW CARD"},{"location":"blogs/week9/#edit-existing-card","text":"","title":"EDIT EXISTING CARD"},{"location":"blogs/week9/#deck-page","text":"A page that shows all the deck in the current game version.","title":"Deck Page"},{"location":"blogs/week9/#deck-editor","text":"All in all, this week saw a lot of progress on building the app. The next week is also going to focus heavily on building the app because of our playtesting deadline. We hope to create something that is simple enough for designers to use!","title":"DECK EDITOR"},{"location":"game/0-structure/","text":"Overview Topics Architecture Unity Player GUI Build of Multiple Frameworks Data Visualization Summary","title":"Overview"},{"location":"game/0-structure/#overview","text":"","title":"Overview"},{"location":"game/0-structure/#topics","text":"","title":"Topics"},{"location":"game/0-structure/#architecture","text":"","title":"Architecture"},{"location":"game/0-structure/#unity-player-gui","text":"","title":"Unity Player GUI"},{"location":"game/0-structure/#build-of-multiple-frameworks","text":"","title":"Build of Multiple Frameworks"},{"location":"game/0-structure/#data-visualization","text":"","title":"Data Visualization"},{"location":"game/0-structure/#summary","text":"","title":"Summary"},{"location":"game/Architecture/","text":"Brief Introduction To Architecture The Requirements The demo we present is how we image game development with AI looks like. It is also a complex software with many subsystems. As an software, it need: Player can play the game AI can play the game Designer can change the game behaviour Automate playtest with AI Visualize the data of the playtest The Architecture Frontend&Backend We divide our architecture into frontend and backend. The backend includes the core of our techique: run the gameplay and train an AI on it. The frontend is all about present the results of backend to user: player GUI present the gameplay, data visualization present the playtesting data and designer GUI present everything desginers can operate. Six Modules Database The word 'database' is not the database in web such as MySQL, its more like manager of data and records, including AI and gameplay. It's important to seperate data and the logic (gameplay logic) operating those data. Because: 1. We have multiple languages and platforms, we need a persistence layer in this archtecture 2. Much easier to build a tool for designer to configure the game. Gameplay logic We implement the gameplay in python, not C# in Unity. Because we use tensorflow to develop our AI algorithm, this is the better solution. AI Our AI module with APIs from gameplay logic to play and game and train an AI for it. It also provides APIs for Designer GUI, so designer can train AI by just clicking a button, PlayerGUI We seperate logics and graphics of a game. The logic is in python, and graphics is in this part. We use Untiy and because we implement our gameplay in python, this is hard than normal game development in Unity. Data Visualization This module visualize the data from automate playtesting. We find the d3.js this libarary. So we can render different kinds of data with large scale easily. Designer GUI This part is the core of the frontend. It acts like a glue, combine all other modules into a single app, providing GUI for designers.","title":"Architecture"},{"location":"game/Architecture/#brief-introduction-to-architecture","text":"","title":"Brief Introduction To Architecture"},{"location":"game/Architecture/#the-requirements","text":"The demo we present is how we image game development with AI looks like. It is also a complex software with many subsystems. As an software, it need: Player can play the game AI can play the game Designer can change the game behaviour Automate playtest with AI Visualize the data of the playtest","title":"The Requirements"},{"location":"game/Architecture/#the-architecture","text":"","title":"The Architecture"},{"location":"game/Architecture/#frontendbackend","text":"We divide our architecture into frontend and backend. The backend includes the core of our techique: run the gameplay and train an AI on it. The frontend is all about present the results of backend to user: player GUI present the gameplay, data visualization present the playtesting data and designer GUI present everything desginers can operate.","title":"Frontend&amp;Backend"},{"location":"game/Architecture/#six-modules","text":"","title":"Six Modules"},{"location":"game/Architecture/#database","text":"The word 'database' is not the database in web such as MySQL, its more like manager of data and records, including AI and gameplay. It's important to seperate data and the logic (gameplay logic) operating those data. Because: 1. We have multiple languages and platforms, we need a persistence layer in this archtecture 2. Much easier to build a tool for designer to configure the game.","title":"Database"},{"location":"game/Architecture/#gameplay-logic","text":"We implement the gameplay in python, not C# in Unity. Because we use tensorflow to develop our AI algorithm, this is the better solution.","title":"Gameplay logic"},{"location":"game/Architecture/#ai","text":"Our AI module with APIs from gameplay logic to play and game and train an AI for it. It also provides APIs for Designer GUI, so designer can train AI by just clicking a button,","title":"AI"},{"location":"game/Architecture/#playergui","text":"We seperate logics and graphics of a game. The logic is in python, and graphics is in this part. We use Untiy and because we implement our gameplay in python, this is hard than normal game development in Unity.","title":"PlayerGUI"},{"location":"game/Architecture/#data-visualization","text":"This module visualize the data from automate playtesting. We find the d3.js this libarary. So we can render different kinds of data with large scale easily.","title":"Data Visualization"},{"location":"game/Architecture/#designer-gui","text":"This part is the core of the frontend. It acts like a glue, combine all other modules into a single app, providing GUI for designers.","title":"Designer GUI"},{"location":"game/Build/","text":"Build of Multiple Frameworks We successfully packed everything, including unity, python, tensorflow, electron into one executable. So we can distribute our application easily to playtesters. Main Components of Build When we work on unity, unity helps to build everything inside unity. When working on an electron, we use electron-forge to build everything in the electron. However, our application uses both unity,electron and python. And there are no official tools to build them together. Electron native build : We use electron-forge to build resources managed by electrons. The main difference between normal electron build and ours is that there are many resources not managed by electrons, such as unity codes and python codes. Unity executables : There are two unity executables. One is a GUI for playing the game, another is the record playback app. In an ideal situation, these 2 should be combined into one app. Since unity build is very small, build twice is acceptable in size. Python build : We discussed how to build python into unity in previous blogs(Week6). We analyze pros and cons on different approaches. We still use \u201cpython interpreter + source codes\u201d for the same reason Database These are all static files. Just copy it into the right directory. Challenges and Solutions Build Python Environment Even we built our unity app by using \u201cpython interpreter + source codes\u201d for the source codes. We also need to consider the python environment. Provide installer in electron app We put a python installer into the build, and provide the GUI for user to install the python and tensorflow. This didn\u2019t work well when we tested on teammates' computers. One problem is there are many installation configurations, such as version, path,etc. Unknown problem will happen if users don\u2019t configure it in right way. Another problem is that some teammates already have installed the python in computer, but with different version and environment Prebuilt python environment into electron app (Final choice) To make the python environment more stable and convenient, we preinstall the whole python environment, including interpreter and dependencies, into the final executable. Even though the final build grew from 250MB into 1.6GB, the plan is much stable and under control. Build Cross Different Frameworks/Platforms Our project is an example of multiple frameworks and platforms. When we want to distribute our project by binaries, we consider two stages: Prebuild : First stage is to build all modules in their native platforms, as we discussed in Unity, electron, and python. In our case, our biggest challenge is to build python codes. Combination and Communication : When we say combine the build, it's mainly about how to manage the paths, how to read files in the build files. Almost all frameworks(in our case, Unity and Electron) have their own way to get dynamica relative path during runtime, so this is doable. As for the communication, because we use socket to do the inter-process communication. As long as we use the right port, the operating system will handle the rest. Develop Mode VS Runtime Mode Most frameworks have a way separate codes in development and codes in build. We have the same situation, because the way we find codes in other platforms is different between develop mode and build runtime. Now we need to manually redirect the codes(latest developer codes or codes in last version\u2019s build) during the development. This is very inconvenient and very easy to cause bugs. Our solution next step, is to automate the multi-platform build procedure, so that we manage the relationship between develop and build. Manual operation is the devil, we need to wipe this out of our build procedure!","title":"Build of Multiple Frameworks"},{"location":"game/Build/#build-of-multiple-frameworks","text":"We successfully packed everything, including unity, python, tensorflow, electron into one executable. So we can distribute our application easily to playtesters.","title":"Build of Multiple Frameworks"},{"location":"game/Build/#main-components-of-build","text":"When we work on unity, unity helps to build everything inside unity. When working on an electron, we use electron-forge to build everything in the electron. However, our application uses both unity,electron and python. And there are no official tools to build them together. Electron native build : We use electron-forge to build resources managed by electrons. The main difference between normal electron build and ours is that there are many resources not managed by electrons, such as unity codes and python codes. Unity executables : There are two unity executables. One is a GUI for playing the game, another is the record playback app. In an ideal situation, these 2 should be combined into one app. Since unity build is very small, build twice is acceptable in size. Python build : We discussed how to build python into unity in previous blogs(Week6). We analyze pros and cons on different approaches. We still use \u201cpython interpreter + source codes\u201d for the same reason Database These are all static files. Just copy it into the right directory.","title":"Main Components of Build"},{"location":"game/Build/#challenges-and-solutions","text":"","title":"Challenges and Solutions"},{"location":"game/Build/#build-python-environment","text":"Even we built our unity app by using \u201cpython interpreter + source codes\u201d for the source codes. We also need to consider the python environment.","title":"Build Python Environment"},{"location":"game/Build/#provide-installer-in-electron-app","text":"We put a python installer into the build, and provide the GUI for user to install the python and tensorflow. This didn\u2019t work well when we tested on teammates' computers. One problem is there are many installation configurations, such as version, path,etc. Unknown problem will happen if users don\u2019t configure it in right way. Another problem is that some teammates already have installed the python in computer, but with different version and environment","title":"Provide installer in electron app"},{"location":"game/Build/#prebuilt-python-environment-into-electron-app-final-choice","text":"To make the python environment more stable and convenient, we preinstall the whole python environment, including interpreter and dependencies, into the final executable. Even though the final build grew from 250MB into 1.6GB, the plan is much stable and under control.","title":"Prebuilt python environment into electron app (Final choice)"},{"location":"game/Build/#build-cross-different-frameworksplatforms","text":"Our project is an example of multiple frameworks and platforms. When we want to distribute our project by binaries, we consider two stages: Prebuild : First stage is to build all modules in their native platforms, as we discussed in Unity, electron, and python. In our case, our biggest challenge is to build python codes. Combination and Communication : When we say combine the build, it's mainly about how to manage the paths, how to read files in the build files. Almost all frameworks(in our case, Unity and Electron) have their own way to get dynamica relative path during runtime, so this is doable. As for the communication, because we use socket to do the inter-process communication. As long as we use the right port, the operating system will handle the rest.","title":"Build Cross Different Frameworks/Platforms"},{"location":"game/Build/#develop-mode-vs-runtime-mode","text":"Most frameworks have a way separate codes in development and codes in build. We have the same situation, because the way we find codes in other platforms is different between develop mode and build runtime. Now we need to manually redirect the codes(latest developer codes or codes in last version\u2019s build) during the development. This is very inconvenient and very easy to cause bugs. Our solution next step, is to automate the multi-platform build procedure, so that we manage the relationship between develop and build. Manual operation is the devil, we need to wipe this out of our build procedure!","title":"Develop Mode VS Runtime Mode"},{"location":"game/DataVisualization/","text":"Data visualization Data visualization might be the most important part of this app. However, drawing graphics, especially interactive one, is not as same as that in game engines. Naive/Low-level solutions One solution is draw data graphics by using svg, since html standards support this format. Or we can use some libraries like webGL. Even though we can have highly customized graphics to show our data, building a data visualization module from scratch is overscoped because we only have a few weeks to finish the whole app. Javascript Data Visualization Library Javascript has many data visualization libraries. We finally choose d3.js (https://github.com/d3/d3 ) among others for 2 reasons: 1. Independent: Many other data visualization libraries( such as Victory, Rechard,etc) only work with some front-end frameworks like React and Vue. d3.js easter to integrate since it only needs javascript 2. Community resoruces\uff1a Many templates and tutorials of d3.js on the internet.","title":"Data visualization"},{"location":"game/DataVisualization/#data-visualization","text":"Data visualization might be the most important part of this app. However, drawing graphics, especially interactive one, is not as same as that in game engines.","title":"Data visualization"},{"location":"game/DataVisualization/#naivelow-level-solutions","text":"One solution is draw data graphics by using svg, since html standards support this format. Or we can use some libraries like webGL. Even though we can have highly customized graphics to show our data, building a data visualization module from scratch is overscoped because we only have a few weeks to finish the whole app.","title":"Naive/Low-level solutions"},{"location":"game/DataVisualization/#javascript-data-visualization-library","text":"Javascript has many data visualization libraries. We finally choose d3.js (https://github.com/d3/d3 ) among others for 2 reasons: 1. Independent: Many other data visualization libraries( such as Victory, Rechard,etc) only work with some front-end frameworks like React and Vue. d3.js easter to integrate since it only needs javascript 2. Community resoruces\uff1a Many templates and tutorials of d3.js on the internet.","title":"Javascript Data Visualization Library"},{"location":"game/DesktopGUI/","text":"Desktop GUI for designers We plan to develop an app for everything, including AI Module, Unity frontend, python gameplay, card/deck editing, data visualization, etc. Motivations Accessible to designers After half, we start to think about how to help designers. We need a user friendly GUI to let designers use all the tools we provide. Organize our tools chain We have many tools in different platforms using different techniques. When we try to build more and more tools, it is a chance to organize our tool chain. Schemes We have sevarl choices to develop our desktop GUI Unity Built-in UI System Pros Our game GUI is developed in Unity, and we are familiar with Unity! Cons Unity\u2019s UI system is not designed for generalized desktop GUI. This would be a big problem when we try to build complex UI such as data visualization. Browser + HTML + CSS + JS Pros Easy to use, only need a browser. Easy to develop, web techniques are convenient and have many libraries and frameworks. Cons Browsers usually don\u2019t support manipulating local files and start other .exe in another process, which are important to us. Electron(final choice) We finnal chose electron for several reasons: - Use web standards: Developing a desktop app is basically the same as writing a website in electron. It uses html, javascript and css, and we are already familiar with these techniques. - Highlevel, lightweight: Because of html and javascript, developing an app is much easier than other techniques such as windows naive API, Qt, etc.","title":"Desktop GUI for designers"},{"location":"game/DesktopGUI/#desktop-gui-for-designers","text":"We plan to develop an app for everything, including AI Module, Unity frontend, python gameplay, card/deck editing, data visualization, etc.","title":"Desktop GUI for designers"},{"location":"game/DesktopGUI/#motivations","text":"","title":"Motivations"},{"location":"game/DesktopGUI/#accessible-to-designers","text":"After half, we start to think about how to help designers. We need a user friendly GUI to let designers use all the tools we provide.","title":"Accessible to designers"},{"location":"game/DesktopGUI/#organize-our-tools-chain","text":"We have many tools in different platforms using different techniques. When we try to build more and more tools, it is a chance to organize our tool chain.","title":"Organize our tools chain"},{"location":"game/DesktopGUI/#schemes","text":"We have sevarl choices to develop our desktop GUI","title":"Schemes"},{"location":"game/DesktopGUI/#unity-built-in-ui-system","text":"","title":"Unity Built-in UI System"},{"location":"game/DesktopGUI/#pros","text":"Our game GUI is developed in Unity, and we are familiar with Unity!","title":"Pros"},{"location":"game/DesktopGUI/#cons","text":"Unity\u2019s UI system is not designed for generalized desktop GUI. This would be a big problem when we try to build complex UI such as data visualization.","title":"Cons"},{"location":"game/DesktopGUI/#browser-html-css-js","text":"","title":"Browser + HTML + CSS + JS"},{"location":"game/DesktopGUI/#pros_1","text":"Easy to use, only need a browser. Easy to develop, web techniques are convenient and have many libraries and frameworks.","title":"Pros"},{"location":"game/DesktopGUI/#cons_1","text":"Browsers usually don\u2019t support manipulating local files and start other .exe in another process, which are important to us.","title":"Cons"},{"location":"game/DesktopGUI/#electronfinal-choice","text":"We finnal chose electron for several reasons: - Use web standards: Developing a desktop app is basically the same as writing a website in electron. It uses html, javascript and css, and we are already familiar with these techniques. - Highlevel, lightweight: Because of html and javascript, developing an app is much easier than other techniques such as windows naive API, Qt, etc.","title":"Electron(final choice)"},{"location":"game/Overview/","text":"Overview This section is about engineering. During the project, we explore how to integrate AI into the process of traditional game development. We met mang challenges and learned many experiences. So we put the experiences we think can be applied to the general case into this section. If someone asked \u201cI want to develop a game using AI to playtest, how to do this?\u201d, then this section is one part of the answer. AI as tool in game development We treat AI as a tool for the development team. Compared to other tools, this is more complex. The biggest reason making our AI tools so different is that it needs access to gameplay logic. In other words, it requires gameplay programmers to provide formatted APIs to AI, which is challenging because gameplay keeps changing during the development process. Challenges One codes for all situations In our project, we have a game for players, a game for AI, and a game for playtest. In read production, the game itself keeps changing, so we cannot maintain 3 versions of games. Instead, we have one single version that can be played in different situations. Generialization Because we want designers to get quick feedback on his design, we aim to reduce the communication between designers, playtesters and programmers. We need to make our AI and game generalized enough so that we can build a tool, like our demo, designer can use this tool design and get feedback.","title":"Overview"},{"location":"game/Overview/#overview","text":"This section is about engineering. During the project, we explore how to integrate AI into the process of traditional game development. We met mang challenges and learned many experiences. So we put the experiences we think can be applied to the general case into this section. If someone asked \u201cI want to develop a game using AI to playtest, how to do this?\u201d, then this section is one part of the answer.","title":"Overview"},{"location":"game/Overview/#ai-as-tool-in-game-development","text":"We treat AI as a tool for the development team. Compared to other tools, this is more complex. The biggest reason making our AI tools so different is that it needs access to gameplay logic. In other words, it requires gameplay programmers to provide formatted APIs to AI, which is challenging because gameplay keeps changing during the development process.","title":"AI as tool in game development"},{"location":"game/Overview/#challenges","text":"","title":"Challenges"},{"location":"game/Overview/#one-codes-for-all-situations","text":"In our project, we have a game for players, a game for AI, and a game for playtest. In read production, the game itself keeps changing, so we cannot maintain 3 versions of games. Instead, we have one single version that can be played in different situations.","title":"One codes for all situations"},{"location":"game/Overview/#generialization","text":"Because we want designers to get quick feedback on his design, we aim to reduce the communication between designers, playtesters and programmers. We need to make our AI and game generalized enough so that we can build a tool, like our demo, designer can use this tool design and get feedback.","title":"Generialization"},{"location":"game/Summary/","text":"Summary How this architecture solved the challenges mentioned in 'Overview'? We decoupled the gameplay logic to a individual module, so both player GUI, AI, playtesting can use this module with same APIs. And we also try to make our gameplay and AI as generized as possible. After doing this, we separate the everything configurable into a persistence layer. By quering and updateing the data in persistence layer, designer can change the game, train AI and get playtest data without writing codes. What is the biggest difference compared to traditional game development? We found the we write gameplay codes is very different. When we develop a game in game engine such as Unity, we write all gameplay, graphics, physics , VFX,etc in game enigne. However, because our game need to run both with graphic and without graphic. Separate gameplay logics from game engine and put it into another platform and language is challenging. If we want to develop a game with AI to do the playtesting, can we just follow this architecture? Situations could be very different on different games, so it is hard to have one solution can applied to anycase. However, we think that \"six modules\" (AI/ Gameplay/ Database/PlayerGUI/ DataVisualization/ DesignerGUI) represent six general problems every game will meet if it want to use AI to playtest.","title":"Summary"},{"location":"game/Summary/#summary","text":"","title":"Summary"},{"location":"game/Summary/#how-this-architecture-solved-the-challenges-mentioned-in-overview","text":"We decoupled the gameplay logic to a individual module, so both player GUI, AI, playtesting can use this module with same APIs. And we also try to make our gameplay and AI as generized as possible. After doing this, we separate the everything configurable into a persistence layer. By quering and updateing the data in persistence layer, designer can change the game, train AI and get playtest data without writing codes.","title":"How this architecture solved the challenges mentioned in 'Overview'?"},{"location":"game/Summary/#what-is-the-biggest-difference-compared-to-traditional-game-development","text":"We found the we write gameplay codes is very different. When we develop a game in game engine such as Unity, we write all gameplay, graphics, physics , VFX,etc in game enigne. However, because our game need to run both with graphic and without graphic. Separate gameplay logics from game engine and put it into another platform and language is challenging.","title":"What is the biggest difference compared to traditional game development?"},{"location":"game/Summary/#if-we-want-to-develop-a-game-with-ai-to-do-the-playtesting-can-we-just-follow-this-architecture","text":"Situations could be very different on different games, so it is hard to have one solution can applied to anycase. However, we think that \"six modules\" (AI/ Gameplay/ Database/PlayerGUI/ DataVisualization/ DesignerGUI) represent six general problems every game will meet if it want to use AI to playtest.","title":"If we want to develop a game with AI to do the playtesting, can we just follow this architecture?"},{"location":"game/Unity/","text":"Use Unity To Develop the Player GUI Key features of Unity front-end Decoupled front-end : Front-end includes all animations, UI, and graphics, but shouldn\u2019t have anything to control the gameplay logic. Data-driven rendering : Rendering of the game is based on data, just like browsers render the HTML. So when we change game logic in python, the rendering part would change according to the data sent back from python. So that we don\u2019t need to change the coeds to much Art resources hot update : Art-side resources shouldn\u2019t be embedded in the application. All of them are configurable and can be hot updated. So when it provided as tools, result can be immediately seen without restart the application, after we change the art resources Frontend&Backend Architecture The architecture largely inspired by the modern web stack: HTML/Browser/CSS/JS. Because we have a similar situation. Instead of a game application, Unity-side, more like a renderer, it renders the information from the python module, and gathers user input sent back to python. It is very similar to the relationship between browser and server. Frontend Rendering Same as browser render website by HTML file, we have our own \u201cMarkup language\u201d for unity to render. This markup language is much simpler than HTML because it does not try to be general, but only used in this type of game. Biggest difference between HTML and ours is that our markup not only has to consider the static things, but also event sequence, animation. Unity also generates the animation based on the game event in markup. This graphic shows how a markup file in response message is rendered to final elements on the screen. Frontend Animation Challenges In a normal unity game, doing animation is easy since we can get all gameobject instances we want to do animations with. But our game is running in python, all the object instances are stored in python runtime. An object instance sharing mechanism is hard to fit into our current request/response architecture between C#/C++ Solutions We extend our \u201cmarkup-language\u201d to support animation. All the markups in one \u201cgame sequence markup file\u201d share the same ID space, so they can find instance by those ID C#/Python Protocol-based Communication Under this architecture, the communication between C# and Python is very clear, C# send request message to Python, Python send response message to C#. Because request always includes Userinput and response always includes GameSequenceMarkup, we simplified the communication by introducing this architecture!","title":"Unity"},{"location":"game/Unity/#use-unity-to-develop-the-player-gui","text":"","title":"Use Unity To Develop the Player GUI"},{"location":"game/Unity/#key-features-of-unity-front-end","text":"Decoupled front-end : Front-end includes all animations, UI, and graphics, but shouldn\u2019t have anything to control the gameplay logic. Data-driven rendering : Rendering of the game is based on data, just like browsers render the HTML. So when we change game logic in python, the rendering part would change according to the data sent back from python. So that we don\u2019t need to change the coeds to much Art resources hot update : Art-side resources shouldn\u2019t be embedded in the application. All of them are configurable and can be hot updated. So when it provided as tools, result can be immediately seen without restart the application, after we change the art resources","title":"Key features of  Unity front-end"},{"location":"game/Unity/#frontendbackend-architecture","text":"The architecture largely inspired by the modern web stack: HTML/Browser/CSS/JS. Because we have a similar situation. Instead of a game application, Unity-side, more like a renderer, it renders the information from the python module, and gathers user input sent back to python. It is very similar to the relationship between browser and server.","title":"Frontend&amp;Backend Architecture"},{"location":"game/Unity/#frontend-rendering","text":"Same as browser render website by HTML file, we have our own \u201cMarkup language\u201d for unity to render. This markup language is much simpler than HTML because it does not try to be general, but only used in this type of game. Biggest difference between HTML and ours is that our markup not only has to consider the static things, but also event sequence, animation. Unity also generates the animation based on the game event in markup. This graphic shows how a markup file in response message is rendered to final elements on the screen.","title":"Frontend Rendering"},{"location":"game/Unity/#frontend-animation","text":"","title":"Frontend Animation"},{"location":"game/Unity/#challenges","text":"In a normal unity game, doing animation is easy since we can get all gameobject instances we want to do animations with. But our game is running in python, all the object instances are stored in python runtime. An object instance sharing mechanism is hard to fit into our current request/response architecture between C#/C++","title":"Challenges"},{"location":"game/Unity/#solutions","text":"We extend our \u201cmarkup-language\u201d to support animation. All the markups in one \u201cgame sequence markup file\u201d share the same ID space, so they can find instance by those ID","title":"Solutions"},{"location":"game/Unity/#cpython-protocol-based-communication","text":"Under this architecture, the communication between C# and Python is very clear, C# send request message to Python, Python send response message to C#. Because request always includes Userinput and response always includes GameSequenceMarkup, we simplified the communication by introducing this architecture!","title":"C#/Python Protocol-based Communication"}]}